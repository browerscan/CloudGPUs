{"type":"app","meta":{"headers":{"x-nextjs-stale-time":"300","x-nextjs-prerender":"1","x-next-cache-tags":"_N_T_/layout,_N_T_/gpus-over-80gb-vram/layout,_N_T_/gpus-over-80gb-vram/page,_N_T_/gpus-over-80gb-vram"}},"html":"<!DOCTYPE html><!--DTTEuVkNVH1L22DPTtudg--><html lang=\"en\"><head><meta charSet=\"utf-8\"/><meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"/><link rel=\"stylesheet\" href=\"/_next/static/css/8baf7e98a62b946f.css\" data-precedence=\"next\"/><link rel=\"preload\" as=\"script\" fetchPriority=\"low\" href=\"/_next/static/chunks/webpack-74c939c87fa0092a.js\"/><script src=\"/_next/static/chunks/4bd1b696-c023c6e3521b1417.js\" async=\"\"></script><script src=\"/_next/static/chunks/255-cb395327542b56ef.js\" async=\"\"></script><script src=\"/_next/static/chunks/main-app-b0adb8acd5071906.js\" async=\"\"></script><script src=\"/_next/static/chunks/0-662476c4b7ee794e.js\" async=\"\"></script><script src=\"/_next/static/chunks/547-53e2b29717055663.js\" async=\"\"></script><script src=\"/_next/static/chunks/app/layout-de644e7eeb6a0750.js\" async=\"\"></script><script src=\"/_next/static/chunks/299-36a9504c11bdd909.js\" async=\"\"></script><script src=\"/_next/static/chunks/app/gpus-over-80gb-vram/page-8d7dca2fdf3992af.js\" async=\"\"></script><link rel=\"preconnect\" href=\"https://api.cloudgpus.io\"/><link rel=\"dns-prefetch\" href=\"https://api.cloudgpus.io\"/><meta name=\"theme-color\" media=\"(prefers-color-scheme: light)\" content=\"#ffffff\"/><meta name=\"theme-color\" media=\"(prefers-color-scheme: dark)\" content=\"#0b1220\"/><title>GPUs over 80GB VRAM - High Memory Cloud GPU Pricing | CloudGPUs.io</title><meta name=\"description\" content=\"Compare cloud GPUs with over 80GB of VRAM for large language models and high-memory workloads.\"/><link rel=\"author\" href=\"https://cloudgpus.io\"/><meta name=\"author\" content=\"CloudGPUs.io\"/><meta name=\"keywords\" content=\"cloud GPU pricing,GPU cloud comparison,H100 cloud pricing,A100 rental,RTX 4090 cloud,AI training GPU,LLM training cost,GPU-as-a-Service,cloud compute pricing,AI inference GPU,Lambda Labs pricing,RunPod pricing,Vast.ai GPU,CoreWeave GPU\"/><meta name=\"creator\" content=\"CloudGPUs.io\"/><meta name=\"publisher\" content=\"CloudGPUs.io\"/><meta name=\"robots\" content=\"index, follow\"/><meta name=\"googlebot\" content=\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"/><link rel=\"canonical\" href=\"https://cloudgpus.io/gpus-over-80gb-vram\"/><meta property=\"og:title\" content=\"GPUs over 80GB VRAM - High Memory Cloud GPU Pricing\"/><meta property=\"og:description\" content=\"Compare cloud GPUs with over 80GB of VRAM for large language models.\"/><meta property=\"og:url\" content=\"https://cloudgpus.io/gpus-over-80gb-vram\"/><meta name=\"twitter:card\" content=\"summary_large_image\"/><meta name=\"twitter:creator\" content=\"@cloudgpusio\"/><meta name=\"twitter:title\" content=\"CloudGPUs.io — Compare GPU Cloud Prices\"/><meta name=\"twitter:description\" content=\"Compare real-time cloud GPU pricing across 20+ providers. Find the best deals on H100, A100, RTX 4090 and more GPUs for AI training and inference.\"/><meta name=\"twitter:image\" content=\"https://cloudgpus.io/opengraph-image\"/><script type=\"application/ld+json\">{\"@context\":\"https://schema.org\",\"@type\":\"Organization\",\"name\":\"CloudGPUs.io\",\"url\":\"https://cloudgpus.io\",\"logo\":\"https://cloudgpus.io/logo.png\",\"description\":\"Compare on-demand and spot GPU pricing across cloud providers. Find the best deals on H100, A100, RTX 4090 and more GPUs for AI training, inference, and rendering.\",\"sameAs\":[\"https://twitter.com/cloudgpus\",\"https://github.com/cloudgpus\",\"https://www.linkedin.com/company/cloudgpus\"],\"contactPoint\":{\"@type\":\"ContactPoint\",\"contactType\":\"customer service\",\"url\":\"https://cloudgpus.io\"}}</script><script type=\"application/ld+json\">{\"@context\":\"https://schema.org\",\"@type\":\"WebSite\",\"name\":\"CloudGPUs.io\",\"url\":\"https://cloudgpus.io\",\"description\":\"Compare on-demand and spot GPU pricing across cloud providers. Find the best deals on H100, A100, RTX 4090 and more GPUs for AI training, inference, and rendering.\",\"potentialAction\":{\"@type\":\"SearchAction\",\"target\":{\"@type\":\"EntryPoint\",\"urlTemplate\":\"https://cloudgpus.io/cloud-gpu?search={search_term_string}\"},\"query-input\":{\"@type\":\"PropertyValueSpecification\",\"valueRequired\":true,\"valueName\":\"search_term_string\"}}}</script><script src=\"/_next/static/chunks/polyfills-42372ed130431b0a.js\" noModule=\"\"></script></head><body><div hidden=\"\"><!--$--><!--/$--></div><a href=\"#main-content\" class=\"skip-link\">Skip to main content</a><header class=\"card\" style=\"border-radius:0;border-left:0;border-right:0\"><div class=\"container\" style=\"display:flex;gap:16px;align-items:center\"><a style=\"font-weight:800;letter-spacing:-0.02em\" href=\"/\">CloudGPUs.io</a><button class=\"mobile-menu-toggle\" aria-label=\"Toggle navigation menu\" aria-expanded=\"false\"><span></span><span></span><span></span></button><nav aria-label=\"Main navigation\" data-expanded=\"false\" class=\"muted\" style=\"display:flex;gap:12px;font-size:14px\"><a href=\"/cloud-gpu\">GPUs</a><a href=\"/provider\">Providers</a><a href=\"/compare\">Compare</a><a href=\"/best-gpu-for\">Use cases</a><a href=\"/region\">Regions</a><a href=\"/calculator\">Calculator</a></nav><div style=\"margin-left:auto;display:flex;gap:10px;align-items:center\"><button class=\"btn btnSecondary\" style=\"cursor:pointer\">Sign In</button><a class=\"btn btnSecondary\" href=\"https://api.cloudgpus.io/admin\" rel=\"noreferrer\" style=\"font-size:14px\">Admin</a><a class=\"btn\" href=\"/cloud-gpu\">Compare Prices</a></div></div></header><!--$!--><template data-dgst=\"BAILOUT_TO_CLIENT_SIDE_RENDERING\"></template><!--/$--><main id=\"main-content\" tabindex=\"-1\"><div class=\"container\"><script type=\"application/ld+json\">{\"@context\":\"https://schema.org\",\"@type\":\"BreadcrumbList\",\"itemListElement\":[{\"@type\":\"ListItem\",\"position\":1,\"name\":\"Home\",\"item\":\"https://cloudgpus.io/\"},{\"@type\":\"ListItem\",\"position\":2,\"name\":\"GPUs over 80GB VRAM\",\"item\":\"https://cloudgpus.io/gpus-over-80gb-vram\"}]}</script><script type=\"application/ld+json\">{\"@context\":\"https://schema.org\",\"@type\":\"FAQPage\",\"mainEntity\":[{\"@type\":\"Question\",\"name\":\"Which GPUs have over 80GB of VRAM?\",\"acceptedAnswer\":{\"@type\":\"Answer\",\"text\":\"GPUs with 80GB+ VRAM include H100 SXM (80GB), H200 (141GB), A100 80GB, GB200 (192GB), and B200 (192GB per GPU). These high-memory GPUs are essential for training and running large language models.\"}},{\"@type\":\"Question\",\"name\":\"What can you do with 80GB+ VRAM?\",\"acceptedAnswer\":{\"@type\":\"Answer\",\"text\":\"With 80GB+ VRAM, you can train LLMs up to 70B parameters with full precision, run inference on 100B+ parameter models, perform large-scale fine-tuning, and work with high-resolution video generation and 3D rendering workloads.\"}},{\"@type\":\"Question\",\"name\":\"Is 80GB VRAM enough for LLM training?\",\"acceptedAnswer\":{\"@type\":\"Answer\",\"text\":\"80GB VRAM is sufficient for training models up to 70B parameters with techniques like Flash Attention, mixed precision training, and gradient checkpointing. For larger models, you'll need multi-GPU setups with NVLink or model parallelism.\"}}]}</script><div class=\"muted\" style=\"font-size:13px;margin-bottom:16px\"><a style=\"text-decoration:none\" href=\"/\">Home</a> <span style=\"opacity:0.5\">/</span> <span>GPUs over 80GB VRAM</span></div><div class=\"card\" style=\"padding:22px\"><h1 style=\"margin-top:0\">GPUs with 80GB+ VRAM</h1><p class=\"muted\" style=\"max-width:860px;line-height:1.7\">Compare cloud GPUs with 80GB or more of VRAM. High-memory GPUs are essential for training large language models, running inference on massive models, and workloads that require significant memory for data and model weights.</p><div class=\"grid grid4\" style=\"gap:16px;margin-top:18px\"><div class=\"card\" style=\"padding:14px\"><div class=\"muted\" style=\"font-size:12px\">GPU models</div><div style=\"font-weight:800;font-size:20px\">6</div></div><div class=\"card\" style=\"padding:14px\"><div class=\"muted\" style=\"font-size:12px\">From</div><div style=\"font-weight:800;font-size:20px\">$1.79/hr</div></div><div class=\"card\" style=\"padding:14px\"><div class=\"muted\" style=\"font-size:12px\">Max VRAM</div><div style=\"font-weight:800;font-size:20px\">192GB</div></div><div class=\"card\" style=\"padding:14px\"><div class=\"muted\" style=\"font-size:12px\">Providers</div><div style=\"font-weight:800;font-size:20px\">4</div></div></div></div><section class=\"card\" style=\"margin-top:18px;padding:18px\"><h2 style=\"margin-top:0;font-size:18px\">Why High VRAM Matters</h2><div class=\"muted\" style=\"line-height:1.8\"><p style=\"margin-top:0\">GPU memory (VRAM) is often the limiting factor for AI workloads. More VRAM enables:</p><ul style=\"margin-bottom:12px\"><li><strong>Larger batch sizes:</strong> Train with more samples per batch for better gradient estimates and faster convergence</li><li><strong>Bigger models:</strong> Fit more parameters on a single GPU, reducing or eliminating the need for model parallelism</li><li><strong>Longer context windows:</strong> Run inference with larger context lengths for RAG and long-document processing</li><li><strong>Complex workloads:</strong> Handle high-resolution image generation, 3D rendering, and scientific computing datasets</li></ul><p>For LLM training, 80GB VRAM is considered the minimum for comfortable training of 7B-13B parameter models without aggressive optimization. The H100 with 80GB HBM3 and H200 with 141GB HBM3e represent the current standard for production AI infrastructure. The newer Blackwell architecture pushes this further with 192GB of HBM3e per GPU, enabling training of models with 1T+ parameters on fewer GPUs.</p><p style=\"margin-bottom:0\">When selecting a high-VRAM GPU, consider memory bandwidth alongside capacity. HBM3/HBM3e memory provides much higher bandwidth than GDDR6X, which can be just as important as capacity for training speed. Also consider whether the GPU supports NVLink for multi-GPU memory pooling.</p></div></section><div class=\"card\" style=\"margin-top:18px;padding:18px\"><h2 style=\"margin-top:0;font-size:18px\">High-VRAM GPU Pricing</h2><p class=\"muted\" style=\"margin-top:6px;line-height:1.7\">Cloud providers offering GPUs with 80GB+ VRAM. Sorted by lowest price.</p><div style=\"margin-top:12px\"><div><div style=\"display:flex;gap:10px;flex-wrap:wrap;align-items:center;margin-bottom:16px\"><span class=\"muted\" style=\"font-size:13px;font-weight:600\">Quick filters:</span><button class=\"btn btnSecondary\" type=\"button\" style=\"font-size:13px;padding:6px 12px\">Under $2/hr</button><button class=\"btn btnSecondary\" type=\"button\" style=\"font-size:13px;padding:6px 12px\">Enterprise tier only</button><button class=\"btn btnSecondary\" type=\"button\" style=\"font-size:13px;padding:6px 12px\">In stock now</button></div><div class=\"card\" style=\"padding:14px;display:grid;gap:12px;grid-template-columns:repeat(auto-fit, minmax(140px, 1fr))\"><label style=\"display:grid;gap:4px\"><span class=\"muted\" style=\"font-size:12px\">Region</span><select class=\"select\" style=\"font-size:13px\"><option value=\"all\" selected=\"\">All regions</option><option value=\"us-east\">us-east</option></select></label><label style=\"display:grid;gap:4px\"><span class=\"muted\" style=\"font-size:12px\">Network Type</span><select class=\"select\" style=\"font-size:13px\"><option value=\"all\" selected=\"\">All</option><option value=\"infiniband\">InfiniBand only</option><option value=\"nvlink\">NVLink only</option></select></label><label style=\"display:grid;gap:4px\"><span class=\"muted\" style=\"font-size:12px\">Billing Increment</span><select class=\"select\" style=\"font-size:13px\"><option value=\"all\" selected=\"\">All</option><option value=\"per-minute\">Per-minute billing</option><option value=\"per-hour\">Per-hour billing</option></select></label><label style=\"display:grid;gap:4px\"><span class=\"muted\" style=\"font-size:12px\">Availability</span><select class=\"select\" style=\"font-size:13px\"><option value=\"all\" selected=\"\">All</option><option value=\"available\">In stock now</option><option value=\"limited\">Limited/unknown</option></select></label><label style=\"display:grid;gap:4px\"><span class=\"muted\" style=\"font-size:12px\">Max Price ($/GPU-hr)</span><input class=\"input\" type=\"number\" step=\"0.01\" min=\"0\" placeholder=\"No limit\" style=\"font-size:13px\" value=\"\"/></label><label style=\"display:flex;align-items:center;gap:8px;padding-top:18px;cursor:pointer\"><input type=\"checkbox\" style=\"margin:0\"/><span class=\"muted\" style=\"font-size:13px\">Show stale prices</span></label></div><div style=\"margin-top:16px\"><div class=\"card\" style=\"padding:32px;text-align:center;background:rgba(15, 23, 42, 0.03)\"><p class=\"muted\" style=\"margin:0\">No results match your filters. Try adjusting your criteria.</p></div></div></div></div></div><section class=\"card\" style=\"margin-top:18px;padding:18px\"><h2 style=\"margin-top:0;font-size:18px\">GPUs with <!-- -->80-95GB<!-- --> VRAM</h2><div class=\"grid grid3\" style=\"gap:12px\"><a class=\"card\" style=\"padding:14px;text-decoration:none\" href=\"/cloud-gpu/a100-80gb\"><div style=\"font-weight:700\">NVIDIA A100 80GB</div><div class=\"muted\" style=\"font-size:12px;margin-top:4px\">80<!-- -->GB <!-- -->HBM2e<!-- --> · <!-- -->ampere</div><div style=\"margin-top:8px;font-weight:600\">From $<!-- -->1.79<!-- -->/hr</div></a><a class=\"card\" style=\"padding:14px;text-decoration:none\" href=\"/cloud-gpu/h100\"><div style=\"font-weight:700\">NVIDIA H100 SXM</div><div class=\"muted\" style=\"font-size:12px;margin-top:4px\">80<!-- -->GB <!-- -->HBM3<!-- --> · <!-- -->hopper</div><div style=\"margin-top:8px;font-weight:600\">From $<!-- -->2.10<!-- -->/hr</div></a><a class=\"card\" style=\"padding:14px;text-decoration:none\" href=\"/cloud-gpu/h100-pcie\"><div style=\"font-weight:700\">NVIDIA H100 PCIe</div><div class=\"muted\" style=\"font-size:12px;margin-top:4px\">80<!-- -->GB <!-- -->HBM3<!-- --> · <!-- -->hopper</div><div style=\"margin-top:8px;font-weight:600\">From $<!-- -->—<!-- -->/hr</div></a></div></section><section class=\"card\" style=\"margin-top:18px;padding:18px\"><h2 style=\"margin-top:0;font-size:18px\">GPUs with <!-- -->96-143GB<!-- --> VRAM</h2><div class=\"grid grid3\" style=\"gap:12px\"><a class=\"card\" style=\"padding:14px;text-decoration:none\" href=\"/cloud-gpu/h200\"><div style=\"font-weight:700\">NVIDIA H200 SXM</div><div class=\"muted\" style=\"font-size:12px;margin-top:4px\">141<!-- -->GB <!-- -->HBM3e<!-- --> · <!-- -->hopper</div><div style=\"margin-top:8px;font-weight:600\">From $<!-- -->—<!-- -->/hr</div></a></div></section><section class=\"card\" style=\"margin-top:18px;padding:18px\"><h2 style=\"margin-top:0;font-size:18px\">GPUs with <!-- -->144GB+<!-- --> VRAM</h2><div class=\"grid grid3\" style=\"gap:12px\"><a class=\"card\" style=\"padding:14px;text-decoration:none\" href=\"/cloud-gpu/gb200\"><div style=\"font-weight:700\">NVIDIA GB200 NVL72</div><div class=\"muted\" style=\"font-size:12px;margin-top:4px\">192<!-- -->GB <!-- -->HBM3e<!-- --> · <!-- -->blackwell</div><div style=\"margin-top:8px;font-weight:600\">From $<!-- -->—<!-- -->/hr</div></a><a class=\"card\" style=\"padding:14px;text-decoration:none\" href=\"/cloud-gpu/b200\"><div style=\"font-weight:700\">NVIDIA B200 SXM</div><div class=\"muted\" style=\"font-size:12px;margin-top:4px\">192<!-- -->GB <!-- -->HBM3e<!-- --> · <!-- -->blackwell</div><div style=\"margin-top:8px;font-weight:600\">From $<!-- -->—<!-- -->/hr</div></a></div></section><div class=\"grid grid2\" style=\"margin-top:18px\"><section class=\"card\" style=\"padding:18px\"><h2 style=\"margin-top:0;font-size:18px\">VRAM Size Guide</h2><div class=\"muted\" style=\"line-height:1.8\"><p style=\"margin-top:0\"><strong>16-24GB:</strong> Good for inference on 7B models, image generation, fine-tuning small models</p><p><strong>48GB:</strong> Suitable for 13B-30B model inference, medium model fine-tuning</p><p><strong>80GB:</strong> Standard for 70B model training, large-scale inference, and fine-tuning</p><p><strong>141GB+: </strong> Frontier model training, 100B+ parameter models, maximum context lengths</p><p style=\"margin-bottom:0\"><strong>192GB+: </strong> Multi-trillion parameter training, exascale AI, research workloads</p></div></section><section class=\"card\" style=\"padding:18px\"><h2 style=\"margin-top:0;font-size:18px\">Related Filters</h2><div style=\"display:grid;gap:10px\"><a class=\"card\" style=\"padding:12px;text-decoration:none\" href=\"/gpus-with-nvlink\"><div style=\"font-weight:700\">GPUs with NVLink</div><div class=\"muted\" style=\"font-size:12px;margin-top:4px\">High-bandwidth GPU interconnect</div></a><a class=\"card\" style=\"padding:12px;text-decoration:none\" href=\"/gpus-with-infiniband\"><div style=\"font-weight:700\">GPUs with InfiniBand</div><div class=\"muted\" style=\"font-size:12px;margin-top:4px\">Multi-node networking for distributed training</div></a><a class=\"card\" style=\"padding:12px;text-decoration:none\" href=\"/architecture/hopper\"><div style=\"font-weight:700\">Hopper Architecture</div><div class=\"muted\" style=\"font-size:12px;margin-top:4px\">H100, H200 high-VRAM GPUs</div></a></div></section></div></div><!--$--><!--/$--></main><footer class=\"container\" style=\"padding-top:32px;padding-bottom:48px\"><div style=\"display:grid;grid-template-columns:repeat(auto-fit, minmax(200px, 1fr));gap:32px;margin-bottom:24px\"><div><div style=\"font-weight:700;margin-bottom:12px\">GPUs</div><div class=\"muted\" style=\"line-height:1.8;font-size:13px\"><div><a href=\"/cloud-gpu/nvidia-h100\">H100 Pricing</a></div><div><a href=\"/cloud-gpu/nvidia-a100-80gb\">A100 80GB Pricing</a></div><div><a href=\"/cloud-gpu/nvidia-rtx-4090\">RTX 4090 Pricing</a></div><div><a href=\"/cloud-gpu/nvidia-l40s\">L40S Pricing</a></div><div><a href=\"/cloud-gpu\">All GPUs</a></div></div></div><div><div style=\"font-weight:700;margin-bottom:12px\">Use Cases</div><div class=\"muted\" style=\"line-height:1.8;font-size:13px\"><div><a href=\"/best-gpu-for/llm-training\">LLM Training</a></div><div><a href=\"/best-gpu-for/llm-inference\">LLM Inference</a></div><div><a href=\"/best-gpu-for/stable-diffusion\">Stable Diffusion</a></div><div><a href=\"/best-gpu-for/fine-tuning\">Fine-Tuning</a></div><div><a href=\"/best-gpu-for\">All Use Cases</a></div></div></div><div><div style=\"font-weight:700;margin-bottom:12px\">Tools</div><div class=\"muted\" style=\"line-height:1.8;font-size:13px\"><div><a href=\"/calculator/cost-estimator\">Cost Estimator</a></div><div><a href=\"/calculator/gpu-selector\">GPU Selector</a></div><div><a href=\"/calculator/roi-calculator\">ROI Calculator</a></div><div><a href=\"/compare\">Compare Providers</a></div></div></div><div><div style=\"font-weight:700;margin-bottom:12px\">Contact</div><div class=\"muted\" style=\"line-height:1.8;font-size:13px\"><div><a href=\"mailto:hello@cloudgpus.io\">hello@cloudgpus.io</a></div><div style=\"margin-top:8px\">Questions about GPU pricing? Feature requests? We would love to hear from you.</div></div></div></div><div class=\"muted\" style=\"border-top:1px solid rgba(15, 23, 42, 0.08);padding-top:24px;font-size:13px;display:flex;justify-content:space-between;flex-wrap:wrap;gap:16px\"><div><div>© <!-- -->2026<!-- --> CloudGPUs.io. All rights reserved.</div><div style=\"margin-top:4px\">Data is provided as-is. Prices can change frequently; always verify on the provider site.</div></div><div style=\"display:flex;gap:16px\"><a href=\"/cloud-gpu\">GPUs</a><a href=\"/provider\">Providers</a><a href=\"/compare\">Compare</a><a href=\"/region\">Regions</a></div></div></footer><script src=\"/_next/static/chunks/webpack-74c939c87fa0092a.js\" id=\"_R_\" async=\"\"></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,\"1:\\\"$Sreact.fragment\\\"\\n2:I[6535,[\\\"0\\\",\\\"static/chunks/0-662476c4b7ee794e.js\\\",\\\"547\\\",\\\"static/chunks/547-53e2b29717055663.js\\\",\\\"177\\\",\\\"static/chunks/app/layout-de644e7eeb6a0750.js\\\"],\\\"Header\\\"]\\n3:I[9766,[],\\\"\\\"]\\n4:I[8924,[],\\\"\\\"]\\n6:I[2619,[\\\"0\\\",\\\"static/chunks/0-662476c4b7ee794e.js\\\",\\\"299\\\",\\\"static/chunks/299-36a9504c11bdd909.js\\\",\\\"586\\\",\\\"static/chunks/app/gpus-over-80gb-vram/page-8d7dca2fdf3992af.js\\\"],\\\"\\\"]\\ne:I[7150,[],\\\"\\\"]\\n:HL[\\\"/_next/static/css/8baf7e98a62b946f.css\\\",\\\"style\\\"]\\n\"])</script><script>self.__next_f.push([1,\"0:{\\\"P\\\":null,\\\"b\\\":\\\"DTTEuVkNVH1L22DPTtudg\\\",\\\"p\\\":\\\"\\\",\\\"c\\\":[\\\"\\\",\\\"gpus-over-80gb-vram\\\"],\\\"i\\\":false,\\\"f\\\":[[[\\\"\\\",{\\\"children\\\":[\\\"gpus-over-80gb-vram\\\",{\\\"children\\\":[\\\"__PAGE__\\\",{}]}]},\\\"$undefined\\\",\\\"$undefined\\\",true],[\\\"\\\",[\\\"$\\\",\\\"$1\\\",\\\"c\\\",{\\\"children\\\":[[[\\\"$\\\",\\\"link\\\",\\\"0\\\",{\\\"rel\\\":\\\"stylesheet\\\",\\\"href\\\":\\\"/_next/static/css/8baf7e98a62b946f.css\\\",\\\"precedence\\\":\\\"next\\\",\\\"crossOrigin\\\":\\\"$undefined\\\",\\\"nonce\\\":\\\"$undefined\\\"}]],[\\\"$\\\",\\\"html\\\",null,{\\\"lang\\\":\\\"en\\\",\\\"children\\\":[[\\\"$\\\",\\\"head\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"script\\\",null,{\\\"type\\\":\\\"application/ld+json\\\",\\\"dangerouslySetInnerHTML\\\":{\\\"__html\\\":\\\"{\\\\\\\"@context\\\\\\\":\\\\\\\"https://schema.org\\\\\\\",\\\\\\\"@type\\\\\\\":\\\\\\\"Organization\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"CloudGPUs.io\\\\\\\",\\\\\\\"url\\\\\\\":\\\\\\\"https://cloudgpus.io\\\\\\\",\\\\\\\"logo\\\\\\\":\\\\\\\"https://cloudgpus.io/logo.png\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Compare on-demand and spot GPU pricing across cloud providers. Find the best deals on H100, A100, RTX 4090 and more GPUs for AI training, inference, and rendering.\\\\\\\",\\\\\\\"sameAs\\\\\\\":[\\\\\\\"https://twitter.com/cloudgpus\\\\\\\",\\\\\\\"https://github.com/cloudgpus\\\\\\\",\\\\\\\"https://www.linkedin.com/company/cloudgpus\\\\\\\"],\\\\\\\"contactPoint\\\\\\\":{\\\\\\\"@type\\\\\\\":\\\\\\\"ContactPoint\\\\\\\",\\\\\\\"contactType\\\\\\\":\\\\\\\"customer service\\\\\\\",\\\\\\\"url\\\\\\\":\\\\\\\"https://cloudgpus.io\\\\\\\"}}\\\"}}],[\\\"$\\\",\\\"script\\\",null,{\\\"type\\\":\\\"application/ld+json\\\",\\\"dangerouslySetInnerHTML\\\":{\\\"__html\\\":\\\"{\\\\\\\"@context\\\\\\\":\\\\\\\"https://schema.org\\\\\\\",\\\\\\\"@type\\\\\\\":\\\\\\\"WebSite\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"CloudGPUs.io\\\\\\\",\\\\\\\"url\\\\\\\":\\\\\\\"https://cloudgpus.io\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Compare on-demand and spot GPU pricing across cloud providers. Find the best deals on H100, A100, RTX 4090 and more GPUs for AI training, inference, and rendering.\\\\\\\",\\\\\\\"potentialAction\\\\\\\":{\\\\\\\"@type\\\\\\\":\\\\\\\"SearchAction\\\\\\\",\\\\\\\"target\\\\\\\":{\\\\\\\"@type\\\\\\\":\\\\\\\"EntryPoint\\\\\\\",\\\\\\\"urlTemplate\\\\\\\":\\\\\\\"https://cloudgpus.io/cloud-gpu?search={search_term_string}\\\\\\\"},\\\\\\\"query-input\\\\\\\":{\\\\\\\"@type\\\\\\\":\\\\\\\"PropertyValueSpecification\\\\\\\",\\\\\\\"valueRequired\\\\\\\":true,\\\\\\\"valueName\\\\\\\":\\\\\\\"search_term_string\\\\\\\"}}}\\\"}}],[\\\"$\\\",\\\"link\\\",null,{\\\"rel\\\":\\\"preconnect\\\",\\\"href\\\":\\\"https://api.cloudgpus.io\\\"}],[\\\"$\\\",\\\"link\\\",null,{\\\"rel\\\":\\\"dns-prefetch\\\",\\\"href\\\":\\\"https://api.cloudgpus.io\\\"}]]}],[\\\"$\\\",\\\"body\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"a\\\",null,{\\\"href\\\":\\\"#main-content\\\",\\\"className\\\":\\\"skip-link\\\",\\\"children\\\":\\\"Skip to main content\\\"}],[\\\"$\\\",\\\"$L2\\\",null,{}],[\\\"$\\\",\\\"main\\\",null,{\\\"id\\\":\\\"main-content\\\",\\\"tabIndex\\\":-1,\\\"children\\\":[\\\"$\\\",\\\"$L3\\\",null,{\\\"parallelRouterKey\\\":\\\"children\\\",\\\"error\\\":\\\"$undefined\\\",\\\"errorStyles\\\":\\\"$undefined\\\",\\\"errorScripts\\\":\\\"$undefined\\\",\\\"template\\\":[\\\"$\\\",\\\"$L4\\\",null,{}],\\\"templateStyles\\\":\\\"$undefined\\\",\\\"templateScripts\\\":\\\"$undefined\\\",\\\"notFound\\\":[\\\"$L5\\\",[]],\\\"forbidden\\\":\\\"$undefined\\\",\\\"unauthorized\\\":\\\"$undefined\\\"}]}],[\\\"$\\\",\\\"footer\\\",null,{\\\"className\\\":\\\"container\\\",\\\"style\\\":{\\\"paddingTop\\\":32,\\\"paddingBottom\\\":48},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"display\\\":\\\"grid\\\",\\\"gridTemplateColumns\\\":\\\"repeat(auto-fit, minmax(200px, 1fr))\\\",\\\"gap\\\":32,\\\"marginBottom\\\":24},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700,\\\"marginBottom\\\":12},\\\"children\\\":\\\"GPUs\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"lineHeight\\\":1.8,\\\"fontSize\\\":13},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/cloud-gpu/nvidia-h100\\\",\\\"children\\\":\\\"H100 Pricing\\\"}]}],[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/cloud-gpu/nvidia-a100-80gb\\\",\\\"children\\\":\\\"A100 80GB Pricing\\\"}]}],[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/cloud-gpu/nvidia-rtx-4090\\\",\\\"children\\\":\\\"RTX 4090 Pricing\\\"}]}],[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/cloud-gpu/nvidia-l40s\\\",\\\"children\\\":\\\"L40S Pricing\\\"}]}],[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/cloud-gpu\\\",\\\"children\\\":\\\"All GPUs\\\"}]}]]}]]}],[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700,\\\"marginBottom\\\":12},\\\"children\\\":\\\"Use Cases\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"lineHeight\\\":1.8,\\\"fontSize\\\":13},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/best-gpu-for/llm-training\\\",\\\"children\\\":\\\"LLM Training\\\"}]}],[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/best-gpu-for/llm-inference\\\",\\\"children\\\":\\\"LLM Inference\\\"}]}],[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/best-gpu-for/stable-diffusion\\\",\\\"children\\\":\\\"Stable Diffusion\\\"}]}],[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/best-gpu-for/fine-tuning\\\",\\\"children\\\":\\\"Fine-Tuning\\\"}]}],[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/best-gpu-for\\\",\\\"children\\\":\\\"All Use Cases\\\"}]}]]}]]}],[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700,\\\"marginBottom\\\":12},\\\"children\\\":\\\"Tools\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"lineHeight\\\":1.8,\\\"fontSize\\\":13},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/calculator/cost-estimator\\\",\\\"children\\\":\\\"Cost Estimator\\\"}]}],[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/calculator/gpu-selector\\\",\\\"children\\\":\\\"GPU Selector\\\"}]}],[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/calculator/roi-calculator\\\",\\\"children\\\":\\\"ROI Calculator\\\"}]}],\\\"$L7\\\"]}]]}],\\\"$L8\\\"]}],\\\"$L9\\\"]}],\\\"$La\\\"]}]]}]]}],{\\\"children\\\":[\\\"gpus-over-80gb-vram\\\",\\\"$Lb\\\",{\\\"children\\\":[\\\"__PAGE__\\\",\\\"$Lc\\\",{},null,false]},null,false]},null,false],\\\"$Ld\\\",false]],\\\"m\\\":\\\"$undefined\\\",\\\"G\\\":[\\\"$e\\\",[]],\\\"s\\\":false,\\\"S\\\":true}\\n\"])</script><script>self.__next_f.push([1,\"f:I[18,[\\\"0\\\",\\\"static/chunks/0-662476c4b7ee794e.js\\\",\\\"547\\\",\\\"static/chunks/547-53e2b29717055663.js\\\",\\\"177\\\",\\\"static/chunks/app/layout-de644e7eeb6a0750.js\\\"],\\\"CookieConsent\\\"]\\n11:I[4431,[],\\\"OutletBoundary\\\"]\\n13:I[5278,[],\\\"AsyncMetadataOutlet\\\"]\\n15:I[4431,[],\\\"ViewportBoundary\\\"]\\n17:I[4431,[],\\\"MetadataBoundary\\\"]\\n18:\\\"$Sreact.suspense\\\"\\n7:[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/compare\\\",\\\"children\\\":\\\"Compare Providers\\\"}]}]\\n8:[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700,\\\"marginBottom\\\":12},\\\"children\\\":\\\"Contact\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"lineHeight\\\":1.8,\\\"fontSize\\\":13},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"a\\\",null,{\\\"href\\\":\\\"mailto:hello@cloudgpus.io\\\",\\\"children\\\":\\\"hello@cloudgpus.io\\\"}]}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"marginTop\\\":8},\\\"children\\\":\\\"Questions about GPU pricing? Feature requests? We would love to hear from you.\\\"}]]}]]}]\\n\"])</script><script>self.__next_f.push([1,\"9:[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"borderTop\\\":\\\"1px solid rgba(15, 23, 42, 0.08)\\\",\\\"paddingTop\\\":24,\\\"fontSize\\\":13,\\\"display\\\":\\\"flex\\\",\\\"justifyContent\\\":\\\"space-between\\\",\\\"flexWrap\\\":\\\"wrap\\\",\\\"gap\\\":16},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"children\\\":[\\\"© \\\",2026,\\\" CloudGPUs.io. All rights reserved.\\\"]}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"marginTop\\\":4},\\\"children\\\":\\\"Data is provided as-is. Prices can change frequently; always verify on the provider site.\\\"}]]}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"display\\\":\\\"flex\\\",\\\"gap\\\":16},\\\"children\\\":[[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/cloud-gpu\\\",\\\"children\\\":\\\"GPUs\\\"}],[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/provider\\\",\\\"children\\\":\\\"Providers\\\"}],[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/compare\\\",\\\"children\\\":\\\"Compare\\\"}],[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/region\\\",\\\"children\\\":\\\"Regions\\\"}]]}]]}]\\n\"])</script><script>self.__next_f.push([1,\"a:[\\\"$\\\",\\\"$Lf\\\",null,{}]\\nb:[\\\"$\\\",\\\"$1\\\",\\\"c\\\",{\\\"children\\\":[null,[\\\"$\\\",\\\"$L3\\\",null,{\\\"parallelRouterKey\\\":\\\"children\\\",\\\"error\\\":\\\"$undefined\\\",\\\"errorStyles\\\":\\\"$undefined\\\",\\\"errorScripts\\\":\\\"$undefined\\\",\\\"template\\\":[\\\"$\\\",\\\"$L4\\\",null,{}],\\\"templateStyles\\\":\\\"$undefined\\\",\\\"templateScripts\\\":\\\"$undefined\\\",\\\"notFound\\\":\\\"$undefined\\\",\\\"forbidden\\\":\\\"$undefined\\\",\\\"unauthorized\\\":\\\"$undefined\\\"}]]}]\\nc:[\\\"$\\\",\\\"$1\\\",\\\"c\\\",{\\\"children\\\":[\\\"$L10\\\",null,[\\\"$\\\",\\\"$L11\\\",null,{\\\"children\\\":[\\\"$L12\\\",[\\\"$\\\",\\\"$L13\\\",null,{\\\"promise\\\":\\\"$@14\\\"}]]}]]}]\\nd:[\\\"$\\\",\\\"$1\\\",\\\"h\\\",{\\\"children\\\":[null,[[\\\"$\\\",\\\"$L15\\\",null,{\\\"children\\\":\\\"$L16\\\"}],null],[\\\"$\\\",\\\"$L17\\\",null,{\\\"children\\\":[\\\"$\\\",\\\"div\\\",null,{\\\"hidden\\\":true,\\\"children\\\":[\\\"$\\\",\\\"$18\\\",null,{\\\"fallback\\\":null,\\\"children\\\":\\\"$L19\\\"}]}]}]]}]\\n\"])</script><script>self.__next_f.push([1,\"16:[[\\\"$\\\",\\\"meta\\\",\\\"0\\\",{\\\"charSet\\\":\\\"utf-8\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"1\\\",{\\\"name\\\":\\\"viewport\\\",\\\"content\\\":\\\"width=device-width, initial-scale=1\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"2\\\",{\\\"name\\\":\\\"theme-color\\\",\\\"media\\\":\\\"(prefers-color-scheme: light)\\\",\\\"content\\\":\\\"#ffffff\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"3\\\",{\\\"name\\\":\\\"theme-color\\\",\\\"media\\\":\\\"(prefers-color-scheme: dark)\\\",\\\"content\\\":\\\"#0b1220\\\"}]]\\n\"])</script><script>self.__next_f.push([1,\"5:[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"container\\\",\\\"children\\\":[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":48,\\\"textAlign\\\":\\\"center\\\"},\\\"children\\\":[[\\\"$\\\",\\\"h1\\\",null,{\\\"style\\\":{\\\"marginTop\\\":0,\\\"fontSize\\\":48},\\\"children\\\":\\\"404\\\"}],[\\\"$\\\",\\\"h2\\\",null,{\\\"style\\\":{\\\"marginTop\\\":0,\\\"marginBottom\\\":16},\\\"children\\\":\\\"Page not found\\\"}],[\\\"$\\\",\\\"p\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"maxWidth\\\":480,\\\"marginLeft\\\":\\\"auto\\\",\\\"marginRight\\\":\\\"auto\\\",\\\"lineHeight\\\":1.7},\\\"children\\\":\\\"The page you are looking for does not exist. It may have been moved or deleted.\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"marginTop\\\":24,\\\"display\\\":\\\"flex\\\",\\\"gap\\\":12,\\\"justifyContent\\\":\\\"center\\\",\\\"flexWrap\\\":\\\"wrap\\\"},\\\"children\\\":[[\\\"$\\\",\\\"$L6\\\",null,{\\\"className\\\":\\\"btn\\\",\\\"href\\\":\\\"/\\\",\\\"children\\\":\\\"Go to homepage\\\"}],[\\\"$\\\",\\\"$L6\\\",null,{\\\"className\\\":\\\"btn btnSecondary\\\",\\\"href\\\":\\\"/cloud-gpu\\\",\\\"children\\\":\\\"Browse all GPUs\\\"}]]}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"marginTop\\\":40},\\\"children\\\":[[\\\"$\\\",\\\"h3\\\",null,{\\\"style\\\":{\\\"fontSize\\\":16,\\\"marginTop\\\":0,\\\"marginBottom\\\":16},\\\"children\\\":\\\"Popular GPUs\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"grid grid3\\\",\\\"style\\\":{\\\"gap\\\":12},\\\"children\\\":[[\\\"$\\\",\\\"$L6\\\",\\\"gb200-nvl\\\",{\\\"href\\\":\\\"/cloud-gpu/gb200-nvl\\\",\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":14,\\\"textDecoration\\\":\\\"none\\\"},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700},\\\"children\\\":\\\"GB200\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12,\\\"marginTop\\\":4},\\\"children\\\":\\\"View pricing\\\"}]]}],[\\\"$\\\",\\\"$L6\\\",\\\"b200-sxm\\\",{\\\"href\\\":\\\"/cloud-gpu/b200-sxm\\\",\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":14,\\\"textDecoration\\\":\\\"none\\\"},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700},\\\"children\\\":\\\"B200\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12,\\\"marginTop\\\":4},\\\"children\\\":\\\"View pricing\\\"}]]}],[\\\"$\\\",\\\"$L6\\\",\\\"h200-sxm\\\",{\\\"href\\\":\\\"/cloud-gpu/h200-sxm\\\",\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":14,\\\"textDecoration\\\":\\\"none\\\"},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700},\\\"children\\\":\\\"H200\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12,\\\"marginTop\\\":4},\\\"children\\\":\\\"View pricing\\\"}]]}],[\\\"$\\\",\\\"$L6\\\",\\\"a100-80gb\\\",{\\\"href\\\":\\\"/cloud-gpu/a100-80gb\\\",\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":14,\\\"textDecoration\\\":\\\"none\\\"},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700},\\\"children\\\":\\\"A100 80GB\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12,\\\"marginTop\\\":4},\\\"children\\\":\\\"View pricing\\\"}]]}],[\\\"$\\\",\\\"$L6\\\",\\\"h100-sxm\\\",{\\\"href\\\":\\\"/cloud-gpu/h100-sxm\\\",\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":14,\\\"textDecoration\\\":\\\"none\\\"},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700},\\\"children\\\":\\\"H100 SXM\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12,\\\"marginTop\\\":4},\\\"children\\\":\\\"View pricing\\\"}]]}],[\\\"$\\\",\\\"$L6\\\",\\\"h100-pcie\\\",{\\\"href\\\":\\\"/cloud-gpu/h100-pcie\\\",\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":14,\\\"textDecoration\\\":\\\"none\\\"},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700},\\\"children\\\":\\\"H100 PCIe\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12,\\\"marginTop\\\":4},\\\"children\\\":\\\"View pricing\\\"}]]}]]}]]}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"marginTop\\\":32,\\\"paddingTop\\\":24,\\\"borderTop\\\":\\\"1px solid var(--color-border)\\\"},\\\"children\\\":[\\\"$\\\",\\\"p\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":13,\\\"margin\\\":0},\\\"children\\\":[\\\"Looking for something specific? Try our\\\",\\\" \\\",[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/compare\\\",\\\"style\\\":{\\\"textDecoration\\\":\\\"underline\\\"},\\\"children\\\":\\\"comparison tool\\\"}],\\\",\\\",\\\" \\\",[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/best-gpu-for\\\",\\\"style\\\":{\\\"textDecoration\\\":\\\"underline\\\"},\\\"children\\\":\\\"use case guides\\\"}],\\\", or\\\",\\\" \\\",[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/calculator\\\",\\\"style\\\":{\\\"textDecoration\\\":\\\"underline\\\"},\\\"children\\\":\\\"calculators\\\"}],\\\".\\\"]}]}]]}]}]\\n\"])</script><script>self.__next_f.push([1,\"12:null\\n\"])</script><script>self.__next_f.push([1,\"14:{\\\"metadata\\\":[[\\\"$\\\",\\\"title\\\",\\\"0\\\",{\\\"children\\\":\\\"GPUs over 80GB VRAM - High Memory Cloud GPU Pricing | CloudGPUs.io\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"1\\\",{\\\"name\\\":\\\"description\\\",\\\"content\\\":\\\"Compare cloud GPUs with over 80GB of VRAM for large language models and high-memory workloads.\\\"}],[\\\"$\\\",\\\"link\\\",\\\"2\\\",{\\\"rel\\\":\\\"author\\\",\\\"href\\\":\\\"https://cloudgpus.io\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"3\\\",{\\\"name\\\":\\\"author\\\",\\\"content\\\":\\\"CloudGPUs.io\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"4\\\",{\\\"name\\\":\\\"keywords\\\",\\\"content\\\":\\\"cloud GPU pricing,GPU cloud comparison,H100 cloud pricing,A100 rental,RTX 4090 cloud,AI training GPU,LLM training cost,GPU-as-a-Service,cloud compute pricing,AI inference GPU,Lambda Labs pricing,RunPod pricing,Vast.ai GPU,CoreWeave GPU\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"5\\\",{\\\"name\\\":\\\"creator\\\",\\\"content\\\":\\\"CloudGPUs.io\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"6\\\",{\\\"name\\\":\\\"publisher\\\",\\\"content\\\":\\\"CloudGPUs.io\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"7\\\",{\\\"name\\\":\\\"robots\\\",\\\"content\\\":\\\"index, follow\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"8\\\",{\\\"name\\\":\\\"googlebot\\\",\\\"content\\\":\\\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\\\"}],[\\\"$\\\",\\\"link\\\",\\\"9\\\",{\\\"rel\\\":\\\"canonical\\\",\\\"href\\\":\\\"https://cloudgpus.io/gpus-over-80gb-vram\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"10\\\",{\\\"property\\\":\\\"og:title\\\",\\\"content\\\":\\\"GPUs over 80GB VRAM - High Memory Cloud GPU Pricing\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"11\\\",{\\\"property\\\":\\\"og:description\\\",\\\"content\\\":\\\"Compare cloud GPUs with over 80GB of VRAM for large language models.\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"12\\\",{\\\"property\\\":\\\"og:url\\\",\\\"content\\\":\\\"https://cloudgpus.io/gpus-over-80gb-vram\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"13\\\",{\\\"name\\\":\\\"twitter:card\\\",\\\"content\\\":\\\"summary_large_image\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"14\\\",{\\\"name\\\":\\\"twitter:creator\\\",\\\"content\\\":\\\"@cloudgpusio\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"15\\\",{\\\"name\\\":\\\"twitter:title\\\",\\\"content\\\":\\\"CloudGPUs.io — Compare GPU Cloud Prices\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"16\\\",{\\\"name\\\":\\\"twitter:description\\\",\\\"content\\\":\\\"Compare real-time cloud GPU pricing across 20+ providers. Find the best deals on H100, A100, RTX 4090 and more GPUs for AI training and inference.\\\"}],[\\\"$\\\",\\\"meta\\\",\\\"17\\\",{\\\"name\\\":\\\"twitter:image\\\",\\\"content\\\":\\\"https://cloudgpus.io/opengraph-image\\\"}]],\\\"error\\\":null,\\\"digest\\\":\\\"$undefined\\\"}\\n\"])</script><script>self.__next_f.push([1,\"19:\\\"$14:metadata\\\"\\n\"])</script><script>self.__next_f.push([1,\"1a:T424,\"])</script><script>self.__next_f.push([1,\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"FAQPage\\\",\\\"mainEntity\\\":[{\\\"@type\\\":\\\"Question\\\",\\\"name\\\":\\\"Which GPUs have over 80GB of VRAM?\\\",\\\"acceptedAnswer\\\":{\\\"@type\\\":\\\"Answer\\\",\\\"text\\\":\\\"GPUs with 80GB+ VRAM include H100 SXM (80GB), H200 (141GB), A100 80GB, GB200 (192GB), and B200 (192GB per GPU). These high-memory GPUs are essential for training and running large language models.\\\"}},{\\\"@type\\\":\\\"Question\\\",\\\"name\\\":\\\"What can you do with 80GB+ VRAM?\\\",\\\"acceptedAnswer\\\":{\\\"@type\\\":\\\"Answer\\\",\\\"text\\\":\\\"With 80GB+ VRAM, you can train LLMs up to 70B parameters with full precision, run inference on 100B+ parameter models, perform large-scale fine-tuning, and work with high-resolution video generation and 3D rendering workloads.\\\"}},{\\\"@type\\\":\\\"Question\\\",\\\"name\\\":\\\"Is 80GB VRAM enough for LLM training?\\\",\\\"acceptedAnswer\\\":{\\\"@type\\\":\\\"Answer\\\",\\\"text\\\":\\\"80GB VRAM is sufficient for training models up to 70B parameters with techniques like Flash Attention, mixed precision training, and gradient checkpointing. For larger models, you'll need multi-GPU setups with NVLink or model parallelism.\\\"}}]}\"])</script><script>self.__next_f.push([1,\"10:[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"container\\\",\\\"children\\\":[[\\\"$\\\",\\\"script\\\",null,{\\\"type\\\":\\\"application/ld+json\\\",\\\"dangerouslySetInnerHTML\\\":{\\\"__html\\\":\\\"{\\\\\\\"@context\\\\\\\":\\\\\\\"https://schema.org\\\\\\\",\\\\\\\"@type\\\\\\\":\\\\\\\"BreadcrumbList\\\\\\\",\\\\\\\"itemListElement\\\\\\\":[{\\\\\\\"@type\\\\\\\":\\\\\\\"ListItem\\\\\\\",\\\\\\\"position\\\\\\\":1,\\\\\\\"name\\\\\\\":\\\\\\\"Home\\\\\\\",\\\\\\\"item\\\\\\\":\\\\\\\"https://cloudgpus.io/\\\\\\\"},{\\\\\\\"@type\\\\\\\":\\\\\\\"ListItem\\\\\\\",\\\\\\\"position\\\\\\\":2,\\\\\\\"name\\\\\\\":\\\\\\\"GPUs over 80GB VRAM\\\\\\\",\\\\\\\"item\\\\\\\":\\\\\\\"https://cloudgpus.io/gpus-over-80gb-vram\\\\\\\"}]}\\\"}}],[\\\"$\\\",\\\"script\\\",null,{\\\"type\\\":\\\"application/ld+json\\\",\\\"dangerouslySetInnerHTML\\\":{\\\"__html\\\":\\\"$1a\\\"}}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":13,\\\"marginBottom\\\":16},\\\"children\\\":[[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/\\\",\\\"style\\\":{\\\"textDecoration\\\":\\\"none\\\"},\\\"children\\\":\\\"Home\\\"}],\\\" \\\",[\\\"$\\\",\\\"span\\\",null,{\\\"style\\\":{\\\"opacity\\\":0.5},\\\"children\\\":\\\"/\\\"}],\\\" \\\",[\\\"$\\\",\\\"span\\\",null,{\\\"children\\\":\\\"GPUs over 80GB VRAM\\\"}]]}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":22},\\\"children\\\":[[\\\"$\\\",\\\"h1\\\",null,{\\\"style\\\":{\\\"marginTop\\\":0},\\\"children\\\":\\\"GPUs with 80GB+ VRAM\\\"}],[\\\"$\\\",\\\"p\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"maxWidth\\\":860,\\\"lineHeight\\\":1.7},\\\"children\\\":\\\"Compare cloud GPUs with 80GB or more of VRAM. High-memory GPUs are essential for training large language models, running inference on massive models, and workloads that require significant memory for data and model weights.\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"grid grid4\\\",\\\"style\\\":{\\\"gap\\\":16,\\\"marginTop\\\":18},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":14},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12},\\\"children\\\":\\\"GPU models\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":800,\\\"fontSize\\\":20},\\\"children\\\":6}]]}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":14},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12},\\\"children\\\":\\\"From\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":800,\\\"fontSize\\\":20},\\\"children\\\":\\\"$$1.79/hr\\\"}]]}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":14},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12},\\\"children\\\":\\\"Max VRAM\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":800,\\\"fontSize\\\":20},\\\"children\\\":\\\"192GB\\\"}]]}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":14},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12},\\\"children\\\":\\\"Providers\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":800,\\\"fontSize\\\":20},\\\"children\\\":4}]]}]]}]]}],[\\\"$\\\",\\\"section\\\",null,{\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"marginTop\\\":18,\\\"padding\\\":18},\\\"children\\\":[[\\\"$\\\",\\\"h2\\\",null,{\\\"style\\\":{\\\"marginTop\\\":0,\\\"fontSize\\\":18},\\\"children\\\":\\\"Why High VRAM Matters\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"lineHeight\\\":1.8},\\\"children\\\":[[\\\"$\\\",\\\"p\\\",null,{\\\"style\\\":{\\\"marginTop\\\":0},\\\"children\\\":\\\"GPU memory (VRAM) is often the limiting factor for AI workloads. More VRAM enables:\\\"}],[\\\"$\\\",\\\"ul\\\",null,{\\\"style\\\":{\\\"marginBottom\\\":12},\\\"children\\\":[[\\\"$\\\",\\\"li\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"strong\\\",null,{\\\"children\\\":\\\"Larger batch sizes:\\\"}],\\\" Train with more samples per batch for better gradient estimates and faster convergence\\\"]}],[\\\"$\\\",\\\"li\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"strong\\\",null,{\\\"children\\\":\\\"Bigger models:\\\"}],\\\" Fit more parameters on a single GPU, reducing or eliminating the need for model parallelism\\\"]}],[\\\"$\\\",\\\"li\\\",null,{\\\"children\\\":[\\\"$L1b\\\",\\\" Run inference with larger context lengths for RAG and long-document processing\\\"]}],\\\"$L1c\\\"]}],\\\"$L1d\\\",\\\"$L1e\\\"]}]]}],\\\"$L1f\\\",[\\\"$L20\\\",\\\"$L21\\\",\\\"$L22\\\"],\\\"$L23\\\"]}]\\n\"])</script><script>self.__next_f.push([1,\"24:I[6299,[\\\"0\\\",\\\"static/chunks/0-662476c4b7ee794e.js\\\",\\\"299\\\",\\\"static/chunks/299-36a9504c11bdd909.js\\\",\\\"586\\\",\\\"static/chunks/app/gpus-over-80gb-vram/page-8d7dca2fdf3992af.js\\\"],\\\"PriceTable\\\"]\\n1b:[\\\"$\\\",\\\"strong\\\",null,{\\\"children\\\":\\\"Longer context windows:\\\"}]\\n1c:[\\\"$\\\",\\\"li\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"strong\\\",null,{\\\"children\\\":\\\"Complex workloads:\\\"}],\\\" Handle high-resolution image generation, 3D rendering, and scientific computing datasets\\\"]}]\\n1d:[\\\"$\\\",\\\"p\\\",null,{\\\"children\\\":\\\"For LLM training, 80GB VRAM is considered the minimum for comfortable training of 7B-13B parameter models without aggressive optimization. The H100 with 80GB HBM3 and H200 with 141GB HBM3e represent the current standard for production AI infrastructure. The newer Blackwell architecture pushes this further with 192GB of HBM3e per GPU, enabling training of models with 1T+ parameters on fewer GPUs.\\\"}]\\n1e:[\\\"$\\\",\\\"p\\\",null,{\\\"style\\\":{\\\"marginBottom\\\":0},\\\"children\\\":\\\"When selecting a high-VRAM GPU, consider memory bandwidth alongside capacity. HBM3/HBM3e memory provides much higher bandwidth than GDDR6X, which can be just as important as capacity for training speed. Also consider whether the GPU supports NVLink for multi-GPU memory pooling.\\\"}]\\n\"])</script><script>self.__next_f.push([1,\"1f:[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"marginTop\\\":18,\\\"padding\\\":18},\\\"children\\\":[[\\\"$\\\",\\\"h2\\\",null,{\\\"style\\\":{\\\"marginTop\\\":0,\\\"fontSize\\\":18},\\\"children\\\":\\\"High-VRAM GPU Pricing\\\"}],[\\\"$\\\",\\\"p\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"marginTop\\\":6,\\\"lineHeight\\\":1.7},\\\"children\\\":\\\"Cloud providers offering GPUs with 80GB+ VRAM. Sorted by lowest price.\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"marginTop\\\":12},\\\"children\\\":[\\\"$\\\",\\\"$L24\\\",null,{\\\"gpuSlug\\\":\\\"high-vram\\\",\\\"rows\\\":[{\\\"provider\\\":{\\\"slug\\\":\\\"lambda-labs\\\",\\\"name\\\":\\\"Lambda Labs\\\",\\\"displayName\\\":\\\"Lambda\\\",\\\"reliabilityTier\\\":\\\"enterprise\\\",\\\"affiliateUrl\\\":null},\\\"instance\\\":{\\\"instanceType\\\":\\\"a100-80gb\\\",\\\"gpuCount\\\":1,\\\"vcpuCount\\\":null,\\\"ramGb\\\":null,\\\"networkBandwidthGbps\\\":null,\\\"hasNvlink\\\":false,\\\"hasInfiniband\\\":false,\\\"infinibandBandwidthGbps\\\":null,\\\"billingIncrementSeconds\\\":3600,\\\"minRentalHours\\\":1,\\\"regions\\\":[\\\"us-east\\\"]},\\\"onDemand\\\":1.91,\\\"spot\\\":1.4,\\\"availability\\\":\\\"available\\\",\\\"lastUpdated\\\":\\\"2025-12-31T16:00:00.185Z\\\"},{\\\"provider\\\":{\\\"slug\\\":\\\"gcp\\\",\\\"name\\\":\\\"Google Cloud Platform\\\",\\\"displayName\\\":\\\"GCP\\\",\\\"reliabilityTier\\\":\\\"enterprise\\\",\\\"affiliateUrl\\\":null},\\\"instance\\\":{\\\"instanceType\\\":\\\"a100-80gb\\\",\\\"gpuCount\\\":1,\\\"vcpuCount\\\":null,\\\"ramGb\\\":null,\\\"networkBandwidthGbps\\\":null,\\\"hasNvlink\\\":false,\\\"hasInfiniband\\\":false,\\\"infinibandBandwidthGbps\\\":null,\\\"billingIncrementSeconds\\\":3600,\\\"minRentalHours\\\":1,\\\"regions\\\":[\\\"us-east\\\"]},\\\"onDemand\\\":2.13,\\\"spot\\\":1.55,\\\"availability\\\":\\\"available\\\",\\\"lastUpdated\\\":\\\"2025-12-31T16:00:00.185Z\\\"},{\\\"provider\\\":{\\\"slug\\\":\\\"nebius\\\",\\\"name\\\":\\\"Nebius AI\\\",\\\"displayName\\\":\\\"Nebius\\\",\\\"reliabilityTier\\\":\\\"enterprise\\\",\\\"affiliateUrl\\\":null},\\\"instance\\\":{\\\"instanceType\\\":\\\"a100-80gb\\\",\\\"gpuCount\\\":1,\\\"vcpuCount\\\":null,\\\"ramGb\\\":null,\\\"networkBandwidthGbps\\\":null,\\\"hasNvlink\\\":false,\\\"hasInfiniband\\\":false,\\\"infinibandBandwidthGbps\\\":null,\\\"billingIncrementSeconds\\\":3600,\\\"minRentalHours\\\":1,\\\"regions\\\":[\\\"us-east\\\"]},\\\"onDemand\\\":2.13,\\\"spot\\\":1.55,\\\"availability\\\":\\\"available\\\",\\\"lastUpdated\\\":\\\"2025-12-31T16:00:00.188Z\\\"},{\\\"provider\\\":{\\\"slug\\\":\\\"runpod\\\",\\\"name\\\":\\\"RunPod\\\",\\\"displayName\\\":\\\"RunPod\\\",\\\"reliabilityTier\\\":\\\"standard\\\",\\\"affiliateUrl\\\":null},\\\"instance\\\":{\\\"instanceType\\\":\\\"a100-80gb\\\",\\\"gpuCount\\\":1,\\\"vcpuCount\\\":null,\\\"ramGb\\\":null,\\\"networkBandwidthGbps\\\":null,\\\"hasNvlink\\\":false,\\\"hasInfiniband\\\":false,\\\"infinibandBandwidthGbps\\\":null,\\\"billingIncrementSeconds\\\":3600,\\\"minRentalHours\\\":1,\\\"regions\\\":[\\\"us-east\\\"]},\\\"onDemand\\\":1.79,\\\"spot\\\":1.3,\\\"availability\\\":\\\"available\\\",\\\"lastUpdated\\\":\\\"2025-12-31T16:00:00.187Z\\\"},{\\\"provider\\\":{\\\"slug\\\":\\\"lambda-labs\\\",\\\"name\\\":\\\"Lambda Labs\\\",\\\"displayName\\\":\\\"Lambda\\\",\\\"reliabilityTier\\\":\\\"enterprise\\\",\\\"affiliateUrl\\\":null},\\\"instance\\\":{\\\"instanceType\\\":\\\"h100-80gb\\\",\\\"gpuCount\\\":1,\\\"vcpuCount\\\":null,\\\"ramGb\\\":null,\\\"networkBandwidthGbps\\\":null,\\\"hasNvlink\\\":false,\\\"hasInfiniband\\\":false,\\\"infinibandBandwidthGbps\\\":null,\\\"billingIncrementSeconds\\\":3600,\\\"minRentalHours\\\":1,\\\"regions\\\":[\\\"us-east\\\"]},\\\"onDemand\\\":2.25,\\\"spot\\\":1.69,\\\"availability\\\":\\\"available\\\",\\\"lastUpdated\\\":\\\"2025-12-31T16:00:00.185Z\\\"},{\\\"provider\\\":{\\\"slug\\\":\\\"gcp\\\",\\\"name\\\":\\\"Google Cloud Platform\\\",\\\"displayName\\\":\\\"GCP\\\",\\\"reliabilityTier\\\":\\\"enterprise\\\",\\\"affiliateUrl\\\":null},\\\"instance\\\":{\\\"instanceType\\\":\\\"h100-80gb\\\",\\\"gpuCount\\\":1,\\\"vcpuCount\\\":null,\\\"ramGb\\\":null,\\\"networkBandwidthGbps\\\":null,\\\"hasNvlink\\\":false,\\\"hasInfiniband\\\":false,\\\"infinibandBandwidthGbps\\\":null,\\\"billingIncrementSeconds\\\":3600,\\\"minRentalHours\\\":1,\\\"regions\\\":[\\\"us-east\\\"]},\\\"onDemand\\\":2.5,\\\"spot\\\":1.88,\\\"availability\\\":\\\"available\\\",\\\"lastUpdated\\\":\\\"2025-12-31T16:00:00.185Z\\\"},{\\\"provider\\\":{\\\"slug\\\":\\\"nebius\\\",\\\"name\\\":\\\"Nebius AI\\\",\\\"displayName\\\":\\\"Nebius\\\",\\\"reliabilityTier\\\":\\\"enterprise\\\",\\\"affiliateUrl\\\":null},\\\"instance\\\":{\\\"instanceType\\\":\\\"h100-80gb\\\",\\\"gpuCount\\\":1,\\\"vcpuCount\\\":null,\\\"ramGb\\\":null,\\\"networkBandwidthGbps\\\":null,\\\"hasNvlink\\\":false,\\\"hasInfiniband\\\":false,\\\"infinibandBandwidthGbps\\\":null,\\\"billingIncrementSeconds\\\":3600,\\\"minRentalHours\\\":1,\\\"regions\\\":[\\\"us-east\\\"]},\\\"onDemand\\\":2.5,\\\"spot\\\":1.88,\\\"availability\\\":\\\"available\\\",\\\"lastUpdated\\\":\\\"2025-12-31T16:00:00.188Z\\\"},{\\\"provider\\\":{\\\"slug\\\":\\\"runpod\\\",\\\"name\\\":\\\"RunPod\\\",\\\"displayName\\\":\\\"RunPod\\\",\\\"reliabilityTier\\\":\\\"standard\\\",\\\"affiliateUrl\\\":null},\\\"instance\\\":{\\\"instanceType\\\":\\\"h100-80gb\\\",\\\"gpuCount\\\":1,\\\"vcpuCount\\\":null,\\\"ramGb\\\":null,\\\"networkBandwidthGbps\\\":null,\\\"hasNvlink\\\":false,\\\"hasInfiniband\\\":false,\\\"infinibandBandwidthGbps\\\":null,\\\"billingIncrementSeconds\\\":3600,\\\"minRentalHours\\\":1,\\\"regions\\\":[\\\"us-east\\\"]},\\\"onDemand\\\":2.1,\\\"spot\\\":1.58,\\\"availability\\\":\\\"available\\\",\\\"lastUpdated\\\":\\\"2025-12-31T16:00:00.187Z\\\"}]}]}]]}]\\n\"])</script><script>self.__next_f.push([1,\"20:[\\\"$\\\",\\\"section\\\",\\\"80-95GB\\\",{\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"marginTop\\\":18,\\\"padding\\\":18},\\\"children\\\":[[\\\"$\\\",\\\"h2\\\",null,{\\\"style\\\":{\\\"marginTop\\\":0,\\\"fontSize\\\":18},\\\"children\\\":[\\\"GPUs with \\\",\\\"80-95GB\\\",\\\" VRAM\\\"]}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"grid grid3\\\",\\\"style\\\":{\\\"gap\\\":12},\\\"children\\\":[[\\\"$\\\",\\\"$L6\\\",\\\"a100-80gb\\\",{\\\"href\\\":\\\"/cloud-gpu/a100-80gb\\\",\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":14,\\\"textDecoration\\\":\\\"none\\\"},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700},\\\"children\\\":\\\"NVIDIA A100 80GB\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12,\\\"marginTop\\\":4},\\\"children\\\":[80,\\\"GB \\\",\\\"HBM2e\\\",\\\" · \\\",\\\"ampere\\\"]}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"marginTop\\\":8,\\\"fontWeight\\\":600},\\\"children\\\":[\\\"From $\\\",\\\"1.79\\\",\\\"/hr\\\"]}]]}],[\\\"$\\\",\\\"$L6\\\",\\\"h100-sxm\\\",{\\\"href\\\":\\\"/cloud-gpu/h100\\\",\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":14,\\\"textDecoration\\\":\\\"none\\\"},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700},\\\"children\\\":\\\"NVIDIA H100 SXM\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12,\\\"marginTop\\\":4},\\\"children\\\":[80,\\\"GB \\\",\\\"HBM3\\\",\\\" · \\\",\\\"hopper\\\"]}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"marginTop\\\":8,\\\"fontWeight\\\":600},\\\"children\\\":[\\\"From $\\\",\\\"2.10\\\",\\\"/hr\\\"]}]]}],[\\\"$\\\",\\\"$L6\\\",\\\"h100-pcie\\\",{\\\"href\\\":\\\"/cloud-gpu/h100-pcie\\\",\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":14,\\\"textDecoration\\\":\\\"none\\\"},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700},\\\"children\\\":\\\"NVIDIA H100 PCIe\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12,\\\"marginTop\\\":4},\\\"children\\\":[80,\\\"GB \\\",\\\"HBM3\\\",\\\" · \\\",\\\"hopper\\\"]}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"marginTop\\\":8,\\\"fontWeight\\\":600},\\\"children\\\":[\\\"From $\\\",\\\"—\\\",\\\"/hr\\\"]}]]}]]}]]}]\\n\"])</script><script>self.__next_f.push([1,\"21:[\\\"$\\\",\\\"section\\\",\\\"96-143GB\\\",{\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"marginTop\\\":18,\\\"padding\\\":18},\\\"children\\\":[[\\\"$\\\",\\\"h2\\\",null,{\\\"style\\\":{\\\"marginTop\\\":0,\\\"fontSize\\\":18},\\\"children\\\":[\\\"GPUs with \\\",\\\"96-143GB\\\",\\\" VRAM\\\"]}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"grid grid3\\\",\\\"style\\\":{\\\"gap\\\":12},\\\"children\\\":[[\\\"$\\\",\\\"$L6\\\",\\\"h200-sxm\\\",{\\\"href\\\":\\\"/cloud-gpu/h200\\\",\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":14,\\\"textDecoration\\\":\\\"none\\\"},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700},\\\"children\\\":\\\"NVIDIA H200 SXM\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12,\\\"marginTop\\\":4},\\\"children\\\":[141,\\\"GB \\\",\\\"HBM3e\\\",\\\" · \\\",\\\"hopper\\\"]}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"marginTop\\\":8,\\\"fontWeight\\\":600},\\\"children\\\":[\\\"From $\\\",\\\"—\\\",\\\"/hr\\\"]}]]}]]}]]}]\\n\"])</script><script>self.__next_f.push([1,\"22:[\\\"$\\\",\\\"section\\\",\\\"144GB+\\\",{\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"marginTop\\\":18,\\\"padding\\\":18},\\\"children\\\":[[\\\"$\\\",\\\"h2\\\",null,{\\\"style\\\":{\\\"marginTop\\\":0,\\\"fontSize\\\":18},\\\"children\\\":[\\\"GPUs with \\\",\\\"144GB+\\\",\\\" VRAM\\\"]}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"grid grid3\\\",\\\"style\\\":{\\\"gap\\\":12},\\\"children\\\":[[\\\"$\\\",\\\"$L6\\\",\\\"gb200-nvl\\\",{\\\"href\\\":\\\"/cloud-gpu/gb200\\\",\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":14,\\\"textDecoration\\\":\\\"none\\\"},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700},\\\"children\\\":\\\"NVIDIA GB200 NVL72\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12,\\\"marginTop\\\":4},\\\"children\\\":[192,\\\"GB \\\",\\\"HBM3e\\\",\\\" · \\\",\\\"blackwell\\\"]}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"marginTop\\\":8,\\\"fontWeight\\\":600},\\\"children\\\":[\\\"From $\\\",\\\"—\\\",\\\"/hr\\\"]}]]}],[\\\"$\\\",\\\"$L6\\\",\\\"b200-sxm\\\",{\\\"href\\\":\\\"/cloud-gpu/b200\\\",\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":14,\\\"textDecoration\\\":\\\"none\\\"},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700},\\\"children\\\":\\\"NVIDIA B200 SXM\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12,\\\"marginTop\\\":4},\\\"children\\\":[192,\\\"GB \\\",\\\"HBM3e\\\",\\\" · \\\",\\\"blackwell\\\"]}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"marginTop\\\":8,\\\"fontWeight\\\":600},\\\"children\\\":[\\\"From $\\\",\\\"—\\\",\\\"/hr\\\"]}]]}]]}]]}]\\n\"])</script><script>self.__next_f.push([1,\"23:[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"grid grid2\\\",\\\"style\\\":{\\\"marginTop\\\":18},\\\"children\\\":[[\\\"$\\\",\\\"section\\\",null,{\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":18},\\\"children\\\":[[\\\"$\\\",\\\"h2\\\",null,{\\\"style\\\":{\\\"marginTop\\\":0,\\\"fontSize\\\":18},\\\"children\\\":\\\"VRAM Size Guide\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"lineHeight\\\":1.8},\\\"children\\\":[[\\\"$\\\",\\\"p\\\",null,{\\\"style\\\":{\\\"marginTop\\\":0},\\\"children\\\":[[\\\"$\\\",\\\"strong\\\",null,{\\\"children\\\":\\\"16-24GB:\\\"}],\\\" Good for inference on 7B models, image generation, fine-tuning small models\\\"]}],[\\\"$\\\",\\\"p\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"strong\\\",null,{\\\"children\\\":\\\"48GB:\\\"}],\\\" Suitable for 13B-30B model inference, medium model fine-tuning\\\"]}],[\\\"$\\\",\\\"p\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"strong\\\",null,{\\\"children\\\":\\\"80GB:\\\"}],\\\" Standard for 70B model training, large-scale inference, and fine-tuning\\\"]}],[\\\"$\\\",\\\"p\\\",null,{\\\"children\\\":[[\\\"$\\\",\\\"strong\\\",null,{\\\"children\\\":\\\"141GB+: \\\"}],\\\" Frontier model training, 100B+ parameter models, maximum context lengths\\\"]}],[\\\"$\\\",\\\"p\\\",null,{\\\"style\\\":{\\\"marginBottom\\\":0},\\\"children\\\":[[\\\"$\\\",\\\"strong\\\",null,{\\\"children\\\":\\\"192GB+: \\\"}],\\\" Multi-trillion parameter training, exascale AI, research workloads\\\"]}]]}]]}],[\\\"$\\\",\\\"section\\\",null,{\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":18},\\\"children\\\":[[\\\"$\\\",\\\"h2\\\",null,{\\\"style\\\":{\\\"marginTop\\\":0,\\\"fontSize\\\":18},\\\"children\\\":\\\"Related Filters\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"display\\\":\\\"grid\\\",\\\"gap\\\":10},\\\"children\\\":[[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/gpus-with-nvlink\\\",\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":12,\\\"textDecoration\\\":\\\"none\\\"},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700},\\\"children\\\":\\\"GPUs with NVLink\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12,\\\"marginTop\\\":4},\\\"children\\\":\\\"High-bandwidth GPU interconnect\\\"}]]}],[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/gpus-with-infiniband\\\",\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":12,\\\"textDecoration\\\":\\\"none\\\"},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700},\\\"children\\\":\\\"GPUs with InfiniBand\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12,\\\"marginTop\\\":4},\\\"children\\\":\\\"Multi-node networking for distributed training\\\"}]]}],[\\\"$\\\",\\\"$L6\\\",null,{\\\"href\\\":\\\"/architecture/hopper\\\",\\\"className\\\":\\\"card\\\",\\\"style\\\":{\\\"padding\\\":12,\\\"textDecoration\\\":\\\"none\\\"},\\\"children\\\":[[\\\"$\\\",\\\"div\\\",null,{\\\"style\\\":{\\\"fontWeight\\\":700},\\\"children\\\":\\\"Hopper Architecture\\\"}],[\\\"$\\\",\\\"div\\\",null,{\\\"className\\\":\\\"muted\\\",\\\"style\\\":{\\\"fontSize\\\":12,\\\"marginTop\\\":4},\\\"children\\\":\\\"H100, H200 high-VRAM GPUs\\\"}]]}]]}]]}]]}]\\n\"])</script></body></html>","rsc":"1:\"$Sreact.fragment\"\n2:I[6535,[\"0\",\"static/chunks/0-662476c4b7ee794e.js\",\"547\",\"static/chunks/547-53e2b29717055663.js\",\"177\",\"static/chunks/app/layout-de644e7eeb6a0750.js\"],\"Header\"]\n3:I[9766,[],\"\"]\n4:I[8924,[],\"\"]\n6:I[2619,[\"0\",\"static/chunks/0-662476c4b7ee794e.js\",\"299\",\"static/chunks/299-36a9504c11bdd909.js\",\"586\",\"static/chunks/app/gpus-over-80gb-vram/page-8d7dca2fdf3992af.js\"],\"\"]\ne:I[7150,[],\"\"]\n:HL[\"/_next/static/css/8baf7e98a62b946f.css\",\"style\"]\n0:{\"P\":null,\"b\":\"DTTEuVkNVH1L22DPTtudg\",\"p\":\"\",\"c\":[\"\",\"gpus-over-80gb-vram\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"gpus-over-80gb-vram\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/8baf7e98a62b946f.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"Organization\\\",\\\"name\\\":\\\"CloudGPUs.io\\\",\\\"url\\\":\\\"https://cloudgpus.io\\\",\\\"logo\\\":\\\"https://cloudgpus.io/logo.png\\\",\\\"description\\\":\\\"Compare on-demand and spot GPU pricing across cloud providers. Find the best deals on H100, A100, RTX 4090 and more GPUs for AI training, inference, and rendering.\\\",\\\"sameAs\\\":[\\\"https://twitter.com/cloudgpus\\\",\\\"https://github.com/cloudgpus\\\",\\\"https://www.linkedin.com/company/cloudgpus\\\"],\\\"contactPoint\\\":{\\\"@type\\\":\\\"ContactPoint\\\",\\\"contactType\\\":\\\"customer service\\\",\\\"url\\\":\\\"https://cloudgpus.io\\\"}}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"WebSite\\\",\\\"name\\\":\\\"CloudGPUs.io\\\",\\\"url\\\":\\\"https://cloudgpus.io\\\",\\\"description\\\":\\\"Compare on-demand and spot GPU pricing across cloud providers. Find the best deals on H100, A100, RTX 4090 and more GPUs for AI training, inference, and rendering.\\\",\\\"potentialAction\\\":{\\\"@type\\\":\\\"SearchAction\\\",\\\"target\\\":{\\\"@type\\\":\\\"EntryPoint\\\",\\\"urlTemplate\\\":\\\"https://cloudgpus.io/cloud-gpu?search={search_term_string}\\\"},\\\"query-input\\\":{\\\"@type\\\":\\\"PropertyValueSpecification\\\",\\\"valueRequired\\\":true,\\\"valueName\\\":\\\"search_term_string\\\"}}}\"}}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://api.cloudgpus.io\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://api.cloudgpus.io\"}]]}],[\"$\",\"body\",null,{\"children\":[[\"$\",\"a\",null,{\"href\":\"#main-content\",\"className\":\"skip-link\",\"children\":\"Skip to main content\"}],[\"$\",\"$L2\",null,{}],[\"$\",\"main\",null,{\"id\":\"main-content\",\"tabIndex\":-1,\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[\"$L5\",[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"footer\",null,{\"className\":\"container\",\"style\":{\"paddingTop\":32,\"paddingBottom\":48},\"children\":[[\"$\",\"div\",null,{\"style\":{\"display\":\"grid\",\"gridTemplateColumns\":\"repeat(auto-fit, minmax(200px, 1fr))\",\"gap\":32,\"marginBottom\":24},\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700,\"marginBottom\":12},\"children\":\"GPUs\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"lineHeight\":1.8,\"fontSize\":13},\"children\":[[\"$\",\"div\",null,{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/cloud-gpu/nvidia-h100\",\"children\":\"H100 Pricing\"}]}],[\"$\",\"div\",null,{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/cloud-gpu/nvidia-a100-80gb\",\"children\":\"A100 80GB Pricing\"}]}],[\"$\",\"div\",null,{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/cloud-gpu/nvidia-rtx-4090\",\"children\":\"RTX 4090 Pricing\"}]}],[\"$\",\"div\",null,{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/cloud-gpu/nvidia-l40s\",\"children\":\"L40S Pricing\"}]}],[\"$\",\"div\",null,{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/cloud-gpu\",\"children\":\"All GPUs\"}]}]]}]]}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700,\"marginBottom\":12},\"children\":\"Use Cases\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"lineHeight\":1.8,\"fontSize\":13},\"children\":[[\"$\",\"div\",null,{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/best-gpu-for/llm-training\",\"children\":\"LLM Training\"}]}],[\"$\",\"div\",null,{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/best-gpu-for/llm-inference\",\"children\":\"LLM Inference\"}]}],[\"$\",\"div\",null,{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/best-gpu-for/stable-diffusion\",\"children\":\"Stable Diffusion\"}]}],[\"$\",\"div\",null,{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/best-gpu-for/fine-tuning\",\"children\":\"Fine-Tuning\"}]}],[\"$\",\"div\",null,{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/best-gpu-for\",\"children\":\"All Use Cases\"}]}]]}]]}],[\"$\",\"div\",null,{\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700,\"marginBottom\":12},\"children\":\"Tools\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"lineHeight\":1.8,\"fontSize\":13},\"children\":[[\"$\",\"div\",null,{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/calculator/cost-estimator\",\"children\":\"Cost Estimator\"}]}],[\"$\",\"div\",null,{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/calculator/gpu-selector\",\"children\":\"GPU Selector\"}]}],[\"$\",\"div\",null,{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/calculator/roi-calculator\",\"children\":\"ROI Calculator\"}]}],\"$L7\"]}]]}],\"$L8\"]}],\"$L9\"]}],\"$La\"]}]]}]]}],{\"children\":[\"gpus-over-80gb-vram\",\"$Lb\",{\"children\":[\"__PAGE__\",\"$Lc\",{},null,false]},null,false]},null,false],\"$Ld\",false]],\"m\":\"$undefined\",\"G\":[\"$e\",[]],\"s\":false,\"S\":true}\nf:I[18,[\"0\",\"static/chunks/0-662476c4b7ee794e.js\",\"547\",\"static/chunks/547-53e2b29717055663.js\",\"177\",\"static/chunks/app/layout-de644e7eeb6a0750.js\"],\"CookieConsent\"]\n11:I[4431,[],\"OutletBoundary\"]\n13:I[5278,[],\"AsyncMetadataOutlet\"]\n15:I[4431,[],\"ViewportBoundary\"]\n17:I[4431,[],\"MetadataBoundary\"]\n18:\"$Sreact.suspense\"\n7:[\"$\",\"div\",null,{\"children\":[\"$\",\"$L6\",null,{\"href\":\"/compare\",\"children\":\"Compare Providers\"}]}]\n8:[\"$\",\"div\",null,{\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700,\"marginBottom\":12},\"children\":\"Contact\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"lineHeight\":1.8,\"fontSize\":13},\"children\":[[\"$\",\"div\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"mailto:hello@cloudgpus.io\",\"children\":\"hello@cloudgpus.io\"}]}],[\"$\",\"div\",null,{\"style\":{\"marginTop\":8},\"children\":\"Questions about GPU pricing? Feature requests? We would love to hear from you.\"}]]}]]}]\n9:[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"borderTop\":\"1px solid rgba(15, 23, 42, 0.08)\",\"paddingTop\":24,\"fontSize\":13,\"display\":\"flex\",\"justifyContent\":\"space-between\",\"flexWrap\":\"wrap\",\"gap\":16},\"children\":[[\"$\",\"div\",null,{\"children\":[[\"$\",\"div\",null,{\"children\":[\"© \",2026,\" CloudGPUs.io. All rights reserved.\"]}],[\"$\",\"div\",null,{\"style\":{\"marginTop\":4},\"children\":\"Data is provided as-is. Prices can change frequently; always verify on the provider site.\"}]]}],[\"$\",\"div\",null,{\"style\":{\"display\":\"flex\",\"gap\":16},\"children\":[[\"$\",\"$L6\",null,{\"href\":\"/cloud-gpu\",\"children\":\"GPUs\"}],[\"$\",\"$L6\",null,{\"href\":\"/provider\",\"children\":\"Providers\"}],[\"$\",\"$L6\",null,{\"href\":\"/compare\",\"children\":\"Compare\"}],[\"$\",\"$L6\",null,{\"href\":\"/region\",\"children\":\"Regions\"}]]}]]}]\na:[\"$\",\"$Lf\",null,{}]\nb:[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}]\nc:[\"$\",\"$1\",\"c\",{\"children\":[\"$L10\",null,[\"$\",\"$L11\",null,{\"children\":[\"$L12\",[\"$\",\"$L13\",null,{\"promise\":\"$@14\"}]]}]]}]\nd:[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$L15\",null,{\"children\":\"$L16\"}],null],[\"$\",\"$L17\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$18\",null,{\"fallback\":null,\"children\":\"$L19\"}]}]}]]}]\n16:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"2\",{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: light)\",\"content\":\"#ffffff\"}],[\"$\",\"meta\",\"3\",{\"name\":\"theme-color\",\"media\":\"(prefers-color-scheme: dark)\",\"content\":\"#0b1220\"}]]\n5:[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[\"$\",\"div\",null,{\"className\":\"card\",\"style\":{\"padding\":48,\"textAlign\":\"center\"},\"children\":[[\"$\",\"h1\",null,{\"style\":{\"marginTop\":0,\"fontSize\":48},\"children\":\"404\"}],[\"$\",\"h2\",null,{\"style\":{\"marginTop\":0,\"marginBottom\":16},\"children\":\"Page not found\"}],[\"$\",\"p\",null,{\"className\":\"muted\",\"style\":{\"maxWidth\":480,\"marginLeft\":\"auto\",\"marginRight\":\"auto\",\"lineHeight\":1.7},\"children\":\"The page you are looking for does not exist. It may have been moved or deleted.\"}],[\"$\",\"div\",null,{\"style\":{\"marginTop\":24,\"display\":\"flex\",\"gap\":12,\"justifyContent\":\"center\",\"flexWrap\":\"wrap\"},\"children\":[[\"$\",\"$L6\",null,{\"className\":\"btn\",\"href\":\"/\",\"children\":\"Go to homepage\"}],[\"$\",\"$L6\",null,{\"className\":\"btn btnSecondary\",\"href\":\"/cloud-gpu\",\"children\":\"Browse all GPUs\"}]]}],[\"$\",\"div\",null,{\"style\":{\"marginTop\":40},\"children\":[[\"$\",\"h3\",null,{\"style\":{\"fontSize\":16,\"marginTop\":0,\"marginBottom\":16},\"children\":\"Popular GPUs\"}],[\"$\",\"div\",null,{\"className\":\"grid grid3\",\"style\":{\"gap\":12},\"children\":[[\"$\",\"$L6\",\"gb200-nvl\",{\"href\":\"/cloud-gpu/gb200-nvl\",\"className\":\"card\",\"style\":{\"padding\":14,\"textDecoration\":\"none\"},\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700},\"children\":\"GB200\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12,\"marginTop\":4},\"children\":\"View pricing\"}]]}],[\"$\",\"$L6\",\"b200-sxm\",{\"href\":\"/cloud-gpu/b200-sxm\",\"className\":\"card\",\"style\":{\"padding\":14,\"textDecoration\":\"none\"},\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700},\"children\":\"B200\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12,\"marginTop\":4},\"children\":\"View pricing\"}]]}],[\"$\",\"$L6\",\"h200-sxm\",{\"href\":\"/cloud-gpu/h200-sxm\",\"className\":\"card\",\"style\":{\"padding\":14,\"textDecoration\":\"none\"},\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700},\"children\":\"H200\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12,\"marginTop\":4},\"children\":\"View pricing\"}]]}],[\"$\",\"$L6\",\"a100-80gb\",{\"href\":\"/cloud-gpu/a100-80gb\",\"className\":\"card\",\"style\":{\"padding\":14,\"textDecoration\":\"none\"},\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700},\"children\":\"A100 80GB\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12,\"marginTop\":4},\"children\":\"View pricing\"}]]}],[\"$\",\"$L6\",\"h100-sxm\",{\"href\":\"/cloud-gpu/h100-sxm\",\"className\":\"card\",\"style\":{\"padding\":14,\"textDecoration\":\"none\"},\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700},\"children\":\"H100 SXM\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12,\"marginTop\":4},\"children\":\"View pricing\"}]]}],[\"$\",\"$L6\",\"h100-pcie\",{\"href\":\"/cloud-gpu/h100-pcie\",\"className\":\"card\",\"style\":{\"padding\":14,\"textDecoration\":\"none\"},\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700},\"children\":\"H100 PCIe\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12,\"marginTop\":4},\"children\":\"View pricing\"}]]}]]}]]}],[\"$\",\"div\",null,{\"style\":{\"marginTop\":32,\"paddingTop\":24,\"borderTop\":\"1px solid var(--color-border)\"},\"children\":[\"$\",\"p\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":13,\"margin\":0},\"children\":[\"Looking for something specific? Try our\",\" \",[\"$\",\"$L6\",null,{\"href\":\"/compare\",\"style\":{\"textDecoration\":\"underline\"},\"children\":\"comparison tool\"}],\",\",\" \",[\"$\",\"$L6\",null,{\"href\":\"/best-gpu-for\",\"style\":{\"textDecoration\":\"underline\"},\"children\":\"use case guides\"}],\", or\",\" \",[\"$\",\"$L6\",null,{\"href\":\"/calculator\",\"style\":{\"textDecoration\":\"underline\"},\"children\":\"calculators\"}],\".\"]}]}]]}]}]\n12:null\n14:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"GPUs over 80GB VRAM - High Memory Cloud GPU Pricing | CloudGPUs.io\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Compare cloud GPUs with over 80GB of VRAM for large language models and high-memory workloads.\"}],[\"$\",\"link\",\"2\",{\"rel\":\"author\",\"href\":\"https://cloudgpus.io\"}],[\"$\",\"meta\",\"3\",{\"name\":\"author\",\"content\":\"CloudGPUs.io\"}],[\"$\",\"meta\",\"4\",{\"name\":\"keywords\",\"content\":\"cloud GPU pricing,GPU cloud comparison,H100 cloud pricing,A100 rental,RTX 4090 cloud,AI training GPU,LLM training cost,GPU-as-a-Service,cloud compute pricing,AI inference GPU,Lambda Labs pricing,RunPod pricing,Vast.ai GPU,CoreWeave GPU\"}],[\"$\",\"meta\",\"5\",{\"name\":\"creator\",\"content\":\"CloudGPUs.io\"}],[\"$\",\"meta\",\"6\",{\"name\":\"publisher\",\"content\":\"CloudGPUs.io\"}],[\"$\",\"meta\",\"7\",{\"name\":\"robots\",\"content\":\"index, follow\"}],[\"$\",\"meta\",\"8\",{\"name\":\"googlebot\",\"content\":\"index, follow, max-video-preview:-1, max-image-preview:large, max-snippet:-1\"}],[\"$\",\"link\",\"9\",{\"rel\":\"canonical\",\"href\":\"https://cloudgpus.io/gpus-over-80gb-vram\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:title\",\"content\":\"GPUs over 80GB VRAM - High Memory Cloud GPU Pricing\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:description\",\"content\":\"Compare cloud GPUs with over 80GB of VRAM for large language models.\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:url\",\"content\":\"https://cloudgpus.io/gpus-over-80gb-vram\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"14\",{\"name\":\"twitter:creator\",\"content\":\"@cloudgpusio\"}],[\"$\",\"meta\",\"15\",{\"name\":\"twitter:title\",\"content\":\"CloudGPUs.io — Compare GPU Cloud Prices\"}],[\"$\",\"meta\",\"16\",{\"name\":\"twitter:description\",\"content\":\"Compare real-time cloud GPU pricing across 20+ providers. Find the best deals on H100, A100, RTX 4090 and more GPUs for AI training and inference.\"}],[\"$\",\"meta\",\"17\",{\"name\":\"twitter:image\",\"content\":\"https://cloudgpus.io/opengraph-image\"}]],\"error\":null,\"digest\":\"$undefined\"}\n19:\"$14:metadata\"\n1a:T424,{\"@context\":\"https://schema.org\",\"@type\":\"FAQPage\",\"mainEntity\":[{\"@type\":\"Question\",\"name\":\"Which GPUs have over 80GB of VRAM?\",\"acceptedAnswer\":{\"@type\":\"Answer\",\"text\":\"GPUs with 80GB+ VRAM include H100 SXM (80GB), H200 (141GB), A100 80GB, GB200 (192GB), and B200 (192GB per GPU). These high-memory GPUs are essential for training and running large language models.\"}},{\"@type\":\"Question\",\"name\":\"What can you do with 80GB+ VRAM?\",\"acceptedAnswer\":{\"@type\":\"Answer\",\"text\":\"With 80GB+ VRAM, you can train LLMs up to 70B parameters with full precision, run inference on 100B+ parameter models, perform large-scale fine-tuning, and work with high-resolution video generation and 3D rendering workloads.\"}},{\"@type\":\"Question\",\"name\":\"Is 80GB VRAM enough for LLM training?\",\"acceptedAnswer\":{\"@type\":\"Answer\",\"text\":\"80GB VRAM is sufficient for training models up to 70B parameters with techniques like Flash Attention, mixed precision training, and gradient checkpointing. For larger models, you'll need multi-GPU setups with NVLink or model parallelism.\"}}]}10:[\"$\",\"div\",null,{\"className\":\"container\",\"children\":[[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"{\\\"@context\\\":\\\"https://schema.org\\\",\\\"@type\\\":\\\"BreadcrumbList\\\",\\\"itemListElement\\\":[{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":1,\\\"name\\\":\\\"Home\\\",\\\"item\\\":\\\"https://cloudgpus.io/\\\"},{\\\"@type\\\":\\\"ListItem\\\",\\\"position\\\":2,\\\"name\\\":\\\"GPUs over 80GB VRAM\\\",\\\"item\\\":\\\"https://cloudgpus.io/gpus-over-80gb-vram\\\"}]}\"}}],[\"$\",\"script\",null,{\"type\":\"application/ld+json\",\"dangerouslySetInnerHTML\":{\"__html\":\"$1a\"}}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":13,\"marginBottom\":16},\"children\":[[\"$\",\"$L6\",null,{\"href\":\"/\",\"style\":{\"textDecoration\":\"none\"},\"children\":\"Home\"}],\" \",[\"$\",\"span\",null,{\"style\":{\"opacity\":0.5},\"children\":\"/\"}],\" \",[\"$\",\"span\",null,{\"children\":\"GPUs over 80GB VRAM\"}]]}],[\"$\",\"div\",null,{\"className\":\"card\",\"style\":{\"padding\":22},\"children\":[[\"$\",\"h1\",null,{\"style\":{\"marginTop\":0},\"children\":\"GPUs with 80GB+ VRAM\"}],[\"$\",\"p\",null,{\"className\":\"muted\",\"style\":{\"maxWidth\":860,\"lineHeight\":1.7},\"children\":\"Compare cloud GPUs with 80GB or more of VRAM. High-memory GPUs are essential for training large language models, running inference on massive models, and workloads that require significant memory for data and model weights.\"}],[\"$\",\"div\",null,{\"className\":\"grid grid4\",\"style\":{\"gap\":16,\"marginTop\":18},\"children\":[[\"$\",\"div\",null,{\"className\":\"card\",\"style\":{\"padding\":14},\"children\":[[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12},\"children\":\"GPU models\"}],[\"$\",\"div\",null,{\"style\":{\"fontWeight\":800,\"fontSize\":20},\"children\":6}]]}],[\"$\",\"div\",null,{\"className\":\"card\",\"style\":{\"padding\":14},\"children\":[[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12},\"children\":\"From\"}],[\"$\",\"div\",null,{\"style\":{\"fontWeight\":800,\"fontSize\":20},\"children\":\"$$1.79/hr\"}]]}],[\"$\",\"div\",null,{\"className\":\"card\",\"style\":{\"padding\":14},\"children\":[[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12},\"children\":\"Max VRAM\"}],[\"$\",\"div\",null,{\"style\":{\"fontWeight\":800,\"fontSize\":20},\"children\":\"192GB\"}]]}],[\"$\",\"div\",null,{\"className\":\"card\",\"style\":{\"padding\":14},\"children\":[[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12},\"children\":\"Providers\"}],[\"$\",\"div\",null,{\"style\":{\"fontWeight\":800,\"fontSize\":20},\"children\":4}]]}]]}]]}],[\"$\",\"section\",null,{\"className\":\"card\",\"style\":{\"marginTop\":18,\"padding\":18},\"children\":[[\"$\",\"h2\",null,{\"style\":{\"marginTop\":0,\"fontSize\":18},\"children\":\"Why High VRAM Matters\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"lineHeight\":1.8},\"children\":[[\"$\",\"p\",null,{\"style\":{\"marginTop\":0},\"children\":\"GPU memory (VRAM) is often the limiting factor for AI workloads. More VRAM enables:\"}],[\"$\",\"ul\",null,{\"style\":{\"marginBottom\":12},\"children\":[[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Larger batch sizes:\"}],\" Train with more samples per batch for better gradient estimates and faster convergence\"]}],[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Bigger models:\"}],\" Fit more parameters on a single GPU, reducing or eliminating the need for model parallelism\"]}],[\"$\",\"li\",null,{\"children\":[\"$L1b\",\" Run inference with larger context lengths for RAG and long-document processing\"]}],\"$L1c\"]}],\"$L1d\",\"$L1e\"]}]]}],\"$L1f\",[\"$L20\",\"$L21\",\"$L22\"],\"$L23\"]}]\n24:I[6299,[\"0\",\"static/chunks/0-662476c4b7ee794e.js\",\"299\",\"static/chunks/299-36a9504c11bdd909.js\",\"586\",\"static/chunks/app/gpus-over-80gb-vram/page-8d7dca2fdf3992af.js\"],\"PriceTable\"]\n1b:[\"$\",\"strong\",null,{\"children\":\"Longer context windows:\"}]\n1c:[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Complex workloads:\"}],\" Handle high-resolution image generation, 3D rendering, and scientific computing datasets\"]}]\n1d:[\"$\",\"p\",null,{\"children\":\"For LLM training, 80GB VRAM is considered the minimum for comfortable training of 7B-13B parameter models without aggressive optimization. The H100 with 80GB HBM3 and H200 with 141GB HBM3e represent the current standard for production AI infrastructure. The newer Blackwell architecture pushes this further with 192GB of HBM3e per GPU, enabling training of models with 1T+ parameters on fewer GPUs.\"}]\n1e:[\"$\",\"p\",null,{\"style\":{\"marginBottom\":0},\"children\":\"When selecting a high-VRAM GPU, consider memory bandwidth alongside capacity. HBM3/HBM3e memory provides much higher bandwidth than GDDR6X, which can be just as important as capacity for training speed. Also consider whether the GPU supports NVLink for multi-GPU memory pooling.\"}]\n1f:[\"$\",\"div\",null,{\"className\":\"card\",\"style\":{\"marginTop\":18,\"padding\":18},\"children\":[[\"$\",\"h2\",null,{\"style\":{\"marginTop\":0,\"fontSize\":18},\"children\":\"High-VRAM GPU Pricing\"}],[\"$\",\"p\",null,{\"className\":\"muted\",\"style\":{\"marginTop\":6,\"lineHeight\":1.7},\"children\":\"Cloud providers offering GPUs with 80GB+ VRAM. Sorted by lowest price.\"}],[\"$\",\"div\",null,{\"style\":{\"marginTop\":12},\"children\":[\"$\",\"$L24\",null,{\"gpuSlug\":\"high-vram\",\"rows\":[{\"provider\":{\"slug\":\"lambda-labs\",\"name\":\"Lambda Labs\",\"displayName\":\"Lambda\",\"reliabilityTier\":\"enterprise\",\"affiliateUrl\":null},\"instance\":{\"instanceType\":\"a100-80gb\",\"gpuCount\":1,\"vcpuCount\":null,\"ramGb\":null,\"networkBandwidthGbps\":null,\"hasNvlink\":false,\"hasInfiniband\":false,\"infinibandBandwidthGbps\":null,\"billingIncrementSeconds\":3600,\"minRentalHours\":1,\"regions\":[\"us-east\"]},\"onDemand\":1.91,\"spot\":1.4,\"availability\":\"available\",\"lastUpdated\":\"2025-12-31T16:00:00.185Z\"},{\"provider\":{\"slug\":\"gcp\",\"name\":\"Google Cloud Platform\",\"displayName\":\"GCP\",\"reliabilityTier\":\"enterprise\",\"affiliateUrl\":null},\"instance\":{\"instanceType\":\"a100-80gb\",\"gpuCount\":1,\"vcpuCount\":null,\"ramGb\":null,\"networkBandwidthGbps\":null,\"hasNvlink\":false,\"hasInfiniband\":false,\"infinibandBandwidthGbps\":null,\"billingIncrementSeconds\":3600,\"minRentalHours\":1,\"regions\":[\"us-east\"]},\"onDemand\":2.13,\"spot\":1.55,\"availability\":\"available\",\"lastUpdated\":\"2025-12-31T16:00:00.185Z\"},{\"provider\":{\"slug\":\"nebius\",\"name\":\"Nebius AI\",\"displayName\":\"Nebius\",\"reliabilityTier\":\"enterprise\",\"affiliateUrl\":null},\"instance\":{\"instanceType\":\"a100-80gb\",\"gpuCount\":1,\"vcpuCount\":null,\"ramGb\":null,\"networkBandwidthGbps\":null,\"hasNvlink\":false,\"hasInfiniband\":false,\"infinibandBandwidthGbps\":null,\"billingIncrementSeconds\":3600,\"minRentalHours\":1,\"regions\":[\"us-east\"]},\"onDemand\":2.13,\"spot\":1.55,\"availability\":\"available\",\"lastUpdated\":\"2025-12-31T16:00:00.188Z\"},{\"provider\":{\"slug\":\"runpod\",\"name\":\"RunPod\",\"displayName\":\"RunPod\",\"reliabilityTier\":\"standard\",\"affiliateUrl\":null},\"instance\":{\"instanceType\":\"a100-80gb\",\"gpuCount\":1,\"vcpuCount\":null,\"ramGb\":null,\"networkBandwidthGbps\":null,\"hasNvlink\":false,\"hasInfiniband\":false,\"infinibandBandwidthGbps\":null,\"billingIncrementSeconds\":3600,\"minRentalHours\":1,\"regions\":[\"us-east\"]},\"onDemand\":1.79,\"spot\":1.3,\"availability\":\"available\",\"lastUpdated\":\"2025-12-31T16:00:00.187Z\"},{\"provider\":{\"slug\":\"lambda-labs\",\"name\":\"Lambda Labs\",\"displayName\":\"Lambda\",\"reliabilityTier\":\"enterprise\",\"affiliateUrl\":null},\"instance\":{\"instanceType\":\"h100-80gb\",\"gpuCount\":1,\"vcpuCount\":null,\"ramGb\":null,\"networkBandwidthGbps\":null,\"hasNvlink\":false,\"hasInfiniband\":false,\"infinibandBandwidthGbps\":null,\"billingIncrementSeconds\":3600,\"minRentalHours\":1,\"regions\":[\"us-east\"]},\"onDemand\":2.25,\"spot\":1.69,\"availability\":\"available\",\"lastUpdated\":\"2025-12-31T16:00:00.185Z\"},{\"provider\":{\"slug\":\"gcp\",\"name\":\"Google Cloud Platform\",\"displayName\":\"GCP\",\"reliabilityTier\":\"enterprise\",\"affiliateUrl\":null},\"instance\":{\"instanceType\":\"h100-80gb\",\"gpuCount\":1,\"vcpuCount\":null,\"ramGb\":null,\"networkBandwidthGbps\":null,\"hasNvlink\":false,\"hasInfiniband\":false,\"infinibandBandwidthGbps\":null,\"billingIncrementSeconds\":3600,\"minRentalHours\":1,\"regions\":[\"us-east\"]},\"onDemand\":2.5,\"spot\":1.88,\"availability\":\"available\",\"lastUpdated\":\"2025-12-31T16:00:00.185Z\"},{\"provider\":{\"slug\":\"nebius\",\"name\":\"Nebius AI\",\"displayName\":\"Nebius\",\"reliabilityTier\":\"enterprise\",\"affiliateUrl\":null},\"instance\":{\"instanceType\":\"h100-80gb\",\"gpuCount\":1,\"vcpuCount\":null,\"ramGb\":null,\"networkBandwidthGbps\":null,\"hasNvlink\":false,\"hasInfiniband\":false,\"infinibandBandwidthGbps\":null,\"billingIncrementSeconds\":3600,\"minRentalHours\":1,\"regions\":[\"us-east\"]},\"onDemand\":2.5,\"spot\":1.88,\"availability\":\"available\",\"lastUpdated\":\"2025-12-31T16:00:00.188Z\"},{\"provider\":{\"slug\":\"runpod\",\"name\":\"RunPod\",\"displayName\":\"RunPod\",\"reliabilityTier\":\"standard\",\"affiliateUrl\":null},\"instance\":{\"instanceType\":\"h100-80gb\",\"gpuCount\":1,\"vcpuCount\":null,\"ramGb\":null,\"networkBandwidthGbps\":null,\"hasNvlink\":false,\"hasInfiniband\":false,\"infinibandBandwidthGbps\":null,\"billingIncrementSeconds\":3600,\"minRentalHours\":1,\"regions\":[\"us-east\"]},\"onDemand\":2.1,\"spot\":1.58,\"availability\":\"available\",\"lastUpdated\":\"2025-12-31T16:00:00.187Z\"}]}]}]]}]\n20:[\"$\",\"section\",\"80-95GB\",{\"className\":\"card\",\"style\":{\"marginTop\":18,\"padding\":18},\"children\":[[\"$\",\"h2\",null,{\"style\":{\"marginTop\":0,\"fontSize\":18},\"children\":[\"GPUs with \",\"80-95GB\",\" VRAM\"]}],[\"$\",\"div\",null,{\"className\":\"grid grid3\",\"style\":{\"gap\":12},\"children\":[[\"$\",\"$L6\",\"a100-80gb\",{\"href\":\"/cloud-gpu/a100-80gb\",\"className\":\"card\",\"style\":{\"padding\":14,\"textDecoration\":\"none\"},\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700},\"children\":\"NVIDIA A100 80GB\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12,\"marginTop\":4},\"children\":[80,\"GB \",\"HBM2e\",\" · \",\"ampere\"]}],[\"$\",\"div\",null,{\"style\":{\"marginTop\":8,\"fontWeight\":600},\"children\":[\"From $\",\"1.79\",\"/hr\"]}]]}],[\"$\",\"$L6\",\"h100-sxm\",{\"href\":\"/cloud-gpu/h100\",\"className\":\"card\",\"style\":{\"padding\":14,\"textDecoration\":\"none\"},\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700},\"children\":\"NVIDIA H100 SXM\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12,\"marginTop\":4},\"children\":[80,\"GB \",\"HBM3\",\" · \",\"hopper\"]}],[\"$\",\"div\",null,{\"style\":{\"marginTop\":8,\"fontWeight\":600},\"children\":[\"From $\",\"2.10\",\"/hr\"]}]]}],[\"$\",\"$L6\",\"h100-pcie\",{\"href\":\"/cloud-gpu/h100-pcie\",\"className\":\"card\",\"style\":{\"padding\":14,\"textDecoration\":\"none\"},\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700},\"children\":\"NVIDIA H100 PCIe\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12,\"marginTop\":4},\"children\":[80,\"GB \",\"HBM3\",\" · \",\"hopper\"]}],[\"$\",\"div\",null,{\"style\":{\"marginTop\":8,\"fontWeight\":600},\"children\":[\"From $\",\"—\",\"/hr\"]}]]}]]}]]}]\n21:[\"$\",\"section\",\"96-143GB\",{\"className\":\"card\",\"style\":{\"marginTop\":18,\"padding\":18},\"children\":[[\"$\",\"h2\",null,{\"style\":{\"marginTop\":0,\"fontSize\":18},\"children\":[\"GPUs with \",\"96-143GB\",\" VRAM\"]}],[\"$\",\"div\",null,{\"className\":\"grid grid3\",\"style\":{\"gap\":12},\"children\":[[\"$\",\"$L6\",\"h200-sxm\",{\"href\":\"/cloud-gpu/h200\",\"className\":\"card\",\"style\":{\"padding\":14,\"textDecoration\":\"none\"},\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700},\"children\":\"NVIDIA H200 SXM\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12,\"marginTop\":4},\"children\":[141,\"GB \",\"HBM3e\",\" · \",\"hopper\"]}],[\"$\",\"div\",null,{\"style\":{\"marginTop\":8,\"fontWeight\":600},\"children\":[\"From $\",\"—\",\"/hr\"]}]]}]]}]]}]\n22:[\"$\",\"section\",\"144GB+\",{\"className\":\"card\",\"style\":{\"marginTop\":18,\"padding\":18},\"children\":[[\"$\",\"h2\",null,{\"style\":{\"marginTop\":0,\"fontSize\":18},\"children\":[\"GPUs with \",\"144GB+\",\" VRAM\"]}],[\"$\",\"div\",null,{\"className\":\"grid grid3\",\"style\":{\"gap\":12},\"children\":[[\"$\",\"$L6\",\"gb200-nvl\",{\"href\":\"/cloud-gpu/gb200\",\"className\":\"card\",\"style\":{\"padding\":14,\"textDecoration\":\"none\"},\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700},\"children\":\"NVIDIA GB200 NVL72\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12,\"marginTop\":4},\"children\":[192,\"GB \",\"HBM3e\",\" · \",\"blackwell\"]}],[\"$\",\"div\",null,{\"style\":{\"marginTop\":8,\"fontWeight\":600},\"children\":[\"From $\",\"—\",\"/hr\"]}]]}],[\"$\",\"$L6\",\"b200-sxm\",{\"href\":\"/cloud-gpu/b200\",\"className\":\"card\",\"style\":{\"padding\":14,\"textDecoration\":\"none\"},\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700},\"children\":\"NVIDIA B200 SXM\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12,\"marginTop\":4},\"children\":[192,\"GB \",\"HBM3e\",\" · \",\"blackwell\"]}],[\"$\",\"div\",null,{\"style\":{\"marginTop\":8,\"fontWeight\":600},\"children\":[\"From $\",\"—\",\"/hr\"]}]]}]]}]]}]\n23:[\"$\",\"div\",null,{\"className\":\"grid grid2\",\"style\":{\"marginTop\":18},\"children\":[[\"$\",\"section\",null,{\"className\":\"card\",\"style\":{\"padding\":18},\"children\":[[\"$\",\"h2\",null,{\"style\":{\"marginTop\":0,\"fontSize\":18},\"children\":\"VRAM Size Guide\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"lineHeight\":1.8},\"children\":[[\"$\",\"p\",null,{\"style\":{\"marginTop\":0},\"children\":[[\"$\",\"strong\",null,{\"children\":\"16-24GB:\"}],\" Good for inference on 7B models, image generation, fine-tuning small models\"]}],[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"48GB:\"}],\" Suitable for 13B-30B model inference, medium model fine-tuning\"]}],[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"80GB:\"}],\" Standard for 70B model training, large-scale inference, and fine-tuning\"]}],[\"$\",\"p\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"141GB+: \"}],\" Frontier model training, 100B+ parameter models, maximum context lengths\"]}],[\"$\",\"p\",null,{\"style\":{\"marginBottom\":0},\"children\":[[\"$\",\"strong\",null,{\"children\":\"192GB+: \"}],\" Multi-trillion parameter training, exascale AI, research workloads\"]}]]}]]}],[\"$\",\"section\",null,{\"className\":\"card\",\"style\":{\"padding\":18},\"children\":[[\"$\",\"h2\",null,{\"style\":{\"marginTop\":0,\"fontSize\":18},\"children\":\"Related Filters\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"grid\",\"gap\":10},\"children\":[[\"$\",\"$L6\",null,{\"href\":\"/gpus-with-nvlink\",\"className\":\"card\",\"style\":{\"padding\":12,\"textDecoration\":\"none\"},\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700},\"children\":\"GPUs with NVLink\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12,\"marginTop\":4},\"children\":\"High-bandwidth GPU interconnect\"}]]}],[\"$\",\"$L6\",null,{\"href\":\"/gpus-with-infiniband\",\"className\":\"card\",\"style\":{\"padding\":12,\"textDecoration\":\"none\"},\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700},\"children\":\"GPUs with InfiniBand\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12,\"marginTop\":4},\"children\":\"Multi-node networking for distributed training\"}]]}],[\"$\",\"$L6\",null,{\"href\":\"/architecture/hopper\",\"className\":\"card\",\"style\":{\"padding\":12,\"textDecoration\":\"none\"},\"children\":[[\"$\",\"div\",null,{\"style\":{\"fontWeight\":700},\"children\":\"Hopper Architecture\"}],[\"$\",\"div\",null,{\"className\":\"muted\",\"style\":{\"fontSize\":12,\"marginTop\":4},\"children\":\"H100, H200 high-VRAM GPUs\"}]]}]]}]]}]]}]\n"}